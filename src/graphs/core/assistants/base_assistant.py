from __future__ import annotations

import logging
from typing import Any
from datetime import datetime

from langchain_core.messages import AIMessage
from langchain_core.runnables import Runnable, RunnableConfig

from src.core.logging_config import log_exception_details
from src.graphs.state.state import RagState


class BaseAssistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def binding_prompt(self, state: RagState) -> dict[str, Any]:
        """Binds state to the prompt, adding necessary context."""
        logging.debug(f"ğŸ” BaseAssistant.binding_prompt - START with state keys: {list(state.keys())}")
        print(f"------------state{state}")
        
        running_summary = ""
        if state.get("context") and isinstance(state["context"], dict):
            summary_obj = state["context"].get("running_summary")
            if summary_obj and hasattr(summary_obj, "summary"):
                running_summary = summary_obj.summary
                logging.debug(f"Running summary found for prompt: {running_summary[:100]}...")

        # FIXED: TÆ°Æ¡ng thÃ­ch vá»›i code cÅ© - state.user chá»©a user_info vÃ  user_profile
        user_data = state.get("user")
        print(f"------------user_data:{user_data}")
        logging.debug(f"ğŸ” BaseAssistant.binding_prompt - user_data type: {type(user_data)}, value: {user_data}")
        
        if user_data and hasattr(user_data, 'user_info') and hasattr(user_data, 'user_profile'):
            # Code cÅ© - user lÃ  User object vá»›i user_info vÃ  user_profile attributes
            user_info = user_data.user_info if hasattr(user_data.user_info, '__dict__') else user_data.user_info.__dict__ if user_data.user_info else {"user_id": "unknown"}
            user_profile = user_data.user_profile if hasattr(user_data.user_profile, '__dict__') else user_data.user_profile.__dict__ if user_data.user_profile else {}
            logging.debug(f"BaseAssistant: Using User object format - user_info: {user_info}, user_profile: {user_profile}")
        elif user_data and isinstance(user_data, dict) and "user_info" in user_data:
            # Code má»›i - user lÃ  dict vá»›i user_info vÃ  user_profile keys (MOST COMMON CASE)
            user_info = user_data.get("user_info", {"user_id": "unknown"})
            user_profile = user_data.get("user_profile", {})
            logging.debug(f"BaseAssistant: Using dict format - user_info: {user_info}, user_profile: {user_profile}")
        elif user_data and isinstance(user_data, dict):
            # Fallback cho dict format khÃ¡c (cÃ³ thá»ƒ cÃ³ trá»±c tiáº¿p user_id)
            user_id = user_data.get("user_id", "unknown")
            user_name = user_data.get("name", "anh/chá»‹")
            user_info = {"user_id": user_id, "name": user_name}
            user_profile = {}
            logging.debug(f"BaseAssistant: Using fallback dict format - user_info: {user_info}")
        else:
            # Fallback cuá»‘i cÃ¹ng - táº¡o defaults tá»« user_id trong state hoáº·c config
            logging.warning(f"No proper user data found in state, user_data: {user_data}, creating defaults from user_id")
            user_id = state.get("user_id", "unknown")
            user_info = {"user_id": user_id, "name": "anh/chá»‹"}
            user_profile = {}
            logging.debug(f"BaseAssistant: Using ultimate fallback - user_info: {user_info}")

        image_contexts = state.get("image_contexts", [])
        if image_contexts:
            logging.info(f"ğŸ–¼ï¸ Binding prompt with {len(image_contexts)} image contexts.")
        
        # CRITICAL: Log state data extraction for DocGrader
        if "document" in state:
            logging.debug(f"ğŸ” BaseAssistant.binding_prompt - found document in state: {state['document'][:100] if state['document'] else 'EMPTY'}...")
        
        prompt = {
            **state,
            "user_info": user_info,
            "user_profile": user_profile,
            "conversation_summary": running_summary,
            "image_contexts": image_contexts,
            # Add default values for common template variables
            "current_date": datetime.now().strftime("%d/%m/%Y"),
            "domain_context": "NhÃ  hÃ ng láº©u bÃ² tÆ°Æ¡i Tian Long",
        }
        
        if not prompt.get("messages"):
            logging.error("No messages found in prompt data during binding.")
            prompt["messages"] = [] # Ensure messages is always a list

        logging.debug(f"ğŸ” BaseAssistant.binding_prompt - FINAL prompt keys: {list(prompt.keys())}")
        return prompt

    def __call__(self, state: RagState, config: RunnableConfig) -> dict[str, Any]:
        """Executes the assistant's runnable."""
        logging.debug(f"ğŸ” BaseAssistant.__call__ - START")
        try:
            # FIXED: TÆ°Æ¡ng thÃ­ch vá»›i cáº£ User object vÃ  dict format
            user_data = state.get("user")
            if user_data and hasattr(user_data, 'user_info'):
                # User object format (code cÅ©)
                user_info = user_data.user_info if hasattr(user_data.user_info, '__dict__') else user_data.user_info.__dict__ if user_data.user_info else {}
                user_id = user_info.get("user_id", "unknown") if isinstance(user_info, dict) else getattr(user_info, "user_id", "unknown")
            elif user_data and isinstance(user_data, dict):
                # Dict format (code má»›i)
                user_info = user_data.get("user_info", {})
                user_id = user_info.get("user_id", "unknown") if user_info else "unknown"
            else:
                # Fallback
                user_id = state.get("user_id", "unknown")
                
            logging.debug(f"ğŸ” BaseAssistant.__call__ - user_id: {user_id}")

            if "configurable" not in config:
                config["configurable"] = {}
            config["configurable"]["user_id"] = user_id

            logging.debug(f"ğŸ” BaseAssistant.__call__ - calling binding_prompt")
            prompt = self.binding_prompt(state)
            
            # CRITICAL: Log the exact prompt data being sent to LLM for DocGrader analysis
            if "DocGrader" in str(type(self)):
                logging.info(f"ğŸ”¬ DOCGRADER PROMPT DATA TO LLM:")
                logging.info(f"   ğŸ“„ document: {prompt.get('document', 'MISSING')[:200] if prompt.get('document') else 'MISSING'}...")
                logging.info(f"   â“ messages: {prompt.get('messages', 'MISSING')}")
                logging.info(f"   ğŸ“ conversation_summary: {prompt.get('conversation_summary', 'MISSING')[:100] if prompt.get('conversation_summary') else 'MISSING'}...")
                logging.info(f"   ğŸ¢ domain_context: {prompt.get('domain_context', 'MISSING')}")
                logging.info(f"   ğŸ“… current_date: {prompt.get('current_date', 'MISSING')}")
                logging.info(f"   ğŸ‘¤ user_info: {prompt.get('user_info', 'MISSING')}")
                logging.info(f"   ğŸ“‹ user_profile: {prompt.get('user_profile', 'MISSING')}")
                
            logging.debug(f"ğŸ” BaseAssistant.__call__ - prompt keys: {list(prompt.keys()) if prompt else 'None'}")
            
            if not prompt or not prompt.get("messages"):
                logging.error("âŒ BaseAssistant: Aborting LLM call due to empty or invalid prompt data.")
                raise ValueError("Prompt data is empty or missing required 'messages' field.")

            logging.debug(f"ğŸ” BaseAssistant.__call__ - invoking runnable")
            result = self.runnable.invoke(prompt, config)
            logging.debug(f"ğŸ” BaseAssistant.__call__ - runnable returned type: {type(result)}")
            logging.debug(f"ğŸ” BaseAssistant.__call__ - runnable returned content: {result}")

            logging.debug(f"ğŸ” BaseAssistant.__call__ - checking if response is valid")
            if self._is_valid_response(result):
                logging.debug("âœ… BaseAssistant: Assistant returned a valid response.")
                return result
            else:
                logging.warning("âš ï¸ BaseAssistant: Assistant returned an invalid or empty response. Providing fallback.")
                fallback = self._create_fallback_response(state)
                logging.debug(f"ğŸ” BaseAssistant.__call__ - fallback response: {fallback}")
                return fallback

        except Exception as e:
            # FIXED: TÆ°Æ¡ng thÃ­ch vá»›i cáº£ User object vÃ  dict format 
            user_data = state.get("user")
            if user_data and hasattr(user_data, 'user_info'):
                user_info = user_data.user_info if hasattr(user_data.user_info, '__dict__') else user_data.user_info.__dict__ if user_data.user_info else {}
                user_id = user_info.get("user_id", "unknown") if isinstance(user_info, dict) else getattr(user_info, "user_id", "unknown")
            elif user_data and isinstance(user_data, dict):
                user_info = user_data.get("user_info", {})
                user_id = user_info.get("user_id", "unknown") if user_info else "unknown"
            else:
                user_id = state.get("user_id", "unknown")
                
            logging.error(f"âŒ BaseAssistant.__call__ - Exception: {type(e).__name__}: {str(e)}")
            log_exception_details(
                exception=e,
                context="Assistant LLM call failed",
                user_id=user_id
            )
            logging.error(f"âŒ BaseAssistant: Assistant exception, providing fallback: {e}")
            fallback = self._create_fallback_response(state)
            logging.debug(f"ğŸ” BaseAssistant.__call__ - exception fallback: {fallback}")
            return fallback

    def _is_valid_response(self, result: Any) -> bool:
        """Checks if the LLM response is meaningful."""
        logging.debug(f"ğŸ” BaseAssistant._is_valid_response - checking result type: {type(result)}")
        
        if hasattr(result, "tool_calls") and result.tool_calls:
            logging.debug(f"âœ… BaseAssistant._is_valid_response - has tool_calls: {len(result.tool_calls)}")
            return True
        
        content = getattr(result, "content", None)
        logging.debug(f"ğŸ” BaseAssistant._is_valid_response - content: {content}")
        
        if not content:
            logging.debug(f"âŒ BaseAssistant._is_valid_response - no content")
            return False
        
        if isinstance(content, str):
            is_valid = content.strip() != ""
            logging.debug(f"ğŸ” BaseAssistant._is_valid_response - string content valid: {is_valid}")
            return is_valid
        
        if isinstance(content, list):
            is_valid = any(
                isinstance(item, dict) and item.get("text", "").strip()
                for item in content
            )
            logging.debug(f"ğŸ” BaseAssistant._is_valid_response - list content valid: {is_valid}")
            return is_valid
        
        is_valid = bool(content)
        logging.debug(f"ğŸ” BaseAssistant._is_valid_response - general content valid: {is_valid}")
        return is_valid

    def _create_fallback_response(self, state: RagState) -> AIMessage:
        """Creates a graceful fallback AIMessage."""
        # FIXED: TÆ°Æ¡ng thÃ­ch vá»›i cáº£ User object vÃ  dict format
        user_data = state.get("user")
        if user_data and hasattr(user_data, 'user_info'):
            # User object format (code cÅ©)
            user_info = user_data.user_info if hasattr(user_data.user_info, '__dict__') else user_data.user_info.__dict__ if user_data.user_info else {}
            user_name = user_info.get("name", "anh/chá»‹") if isinstance(user_info, dict) else getattr(user_info, "name", "anh/chá»‹")
            user_id_for_log = user_info.get("user_id", "unknown") if isinstance(user_info, dict) else getattr(user_info, "user_id", "unknown")
        elif user_data and isinstance(user_data, dict):
            # Dict format (code má»›i)
            user_info = user_data.get("user_info", {})
            user_name = user_info.get("name", "anh/chá»‹")
            user_id_for_log = user_info.get("user_id", "unknown")
        else:
            # Fallback
            user_name = "anh/chá»‹"
            user_id_for_log = "unknown"
        
        fallback_content = (
            f"Xin lá»—i {user_name}, em Ä‘ang gáº·p váº¥n Ä‘á» ká»¹ thuáº­t táº¡m thá»i. "
            f"Vui lÃ²ng thá»­ láº¡i sau hoáº·c liÃªn há»‡ hotline 1900 636 886 Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ trá»±c tiáº¿p. "
            f"Em ráº¥t xin lá»—i vÃ¬ sá»± báº¥t tiá»‡n nÃ y! ğŸ™"
        )
        
        logging.info(f"Providing fallback response for user: {user_id_for_log}")
        
        return AIMessage(
            content=fallback_content,
            additional_kwargs={"fallback_response": True}
        )
