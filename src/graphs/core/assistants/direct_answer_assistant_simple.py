from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from src.graphs.core.assistants.base_assistant import BaseAssistant
from src.graphs.state.state import RagState
from datetime import datetime
from typing import Dict, Any

class DirectAnswerAssistant(BaseAssistant):
    def __init__(self, llm, domain_context, tools):
        self.domain_context = domain_context
        print(f"---------------tools------------------", tools)
        prompt = ChatPromptTemplate.from_messages([
            ("system",
             "B·∫°n l√† Vy - tr·ª£ l√Ω ·∫£o c·ªßa Nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long.\n\n"
             
             "üë§ TH√îNG TIN KH√ÅCH:\n"
             "User: {user_info}\n"
             "Profile: {user_profile}\n"
             "Context: {context}\n\n"
             
             "üîß TOOLS - LU√îN G·ªåI KHI C·∫¶N:\n\n"
             
             "1Ô∏è‚É£ **save_user_preference_with_refresh_flag** - G·ªåI KHI:\n"
             "   ‚Ä¢ T·ª´ kh√≥a: 'th√≠ch', 'y√™u th√≠ch', '∆∞a', 'kh√¥ng th√≠ch', 'gh√©t'\n"
             "   ‚Ä¢ T·ª´ kh√≥a: 'th∆∞·ªùng', 'hay', 'lu√¥n', 'mu·ªën', 'c·∫ßn'\n"
             "   ‚Ä¢ T·ª´ kh√≥a: 'sinh nh·∫≠t'\n"
             "   üìã Input: user_id, preference_type, preference_value\n\n"
             
             "2Ô∏è‚É£ **book_table_reservation** - G·ªåI KHI:\n"
             "   ‚Ä¢ T·ª´ kh√≥a: 'ƒë·∫∑t b√†n', 'book', 'reservation', 'ok ƒë·∫∑t'\n"
             "   ‚Ä¢ CH·ªà sau khi c√≥ ƒë·ªß th√¥ng tin v√† kh√°ch X√ÅC NH·∫¨N\n"
             "   üìã Input: restaurant_location, first_name, last_name, phone, reservation_date, start_time, amount_adult\n\n"
             
             "‚ö° QUY T·∫ÆC TOOL CALLING:\n"
             "‚Ä¢ QU√âT m·ªói tin nh·∫Øn t√¨m keywords ‚Üí G·ªåI TOOL NGAY\n"
             "‚Ä¢ Tool calls HO√ÄN TO√ÄN V√î H√åNH v·ªõi user\n"
             "‚Ä¢ Sau khi g·ªçi tool ‚Üí Tr·∫£ l·ªùi t·ª± nhi√™n\n\n"
             
             "üéØ V√ç D·ª§:\n"
             "User: 'T√¥i th√≠ch ƒÉn cay'\n"
             "‚Üí G·ªåI: save_user_preference_with_refresh_flag('user123', 'food_preference', 'cay')\n"
             "‚Üí TR·∫¢ L·ªúI: 'D·∫° em ƒë√£ ghi nh·ªõ anh th√≠ch ƒÉn cay! üå∂Ô∏è'\n\n"
             
             "User: 'ƒê·∫∑t b√†n cho 4 ng∆∞·ªùi t·ªëi nay l√∫c 7h' (sau khi c√≥ ƒë·ªß info)\n"
             "‚Üí G·ªåI: book_table_reservation(...)\n"
             "‚Üí TR·∫¢ L·ªúI: 'ƒê·∫∑t b√†n th√†nh c√¥ng! üéâ' ho·∫∑c 'Xin l·ªói c√≥ l·ªói...'\n\n"
             
             "ü§ñ PHONG C√ÅCH TR·∫¢ L·ªúI:\n"
             "‚Ä¢ Ng·∫Øn g·ªçn, th√¢n thi·ªán v·ªõi emoji\n"
             "‚Ä¢ S·ª≠ d·ª•ng th√¥ng tin t·ª´ {context} n·∫øu c√≥\n"
             "‚Ä¢ Kh√¥ng nh·∫Øc ƒë·∫øn vi·ªác g·ªçi tools"
            ),
            MessagesPlaceholder(variable_name="messages")
        ])
        
        def get_combined_context(ctx: dict[str, Any]) -> str:
            import logging
            documents = ctx.get("documents", [])
            image_contexts = ctx.get("image_contexts", [])
            
            context_parts = []
            
            # X·ª≠ l√Ω image contexts tr∆∞·ªõc
            if image_contexts:
                for i, img_context in enumerate(image_contexts):
                    if img_context and isinstance(img_context, str):
                        context_parts.append(f"**H√åNH ·∫¢NH {i+1}:**\n{img_context}")
            
            # X·ª≠ l√Ω documents
            if documents:
                for i, doc in enumerate(documents[:5]):  # Ch·ªâ l·∫•y 5 docs ƒë·∫ßu
                    if isinstance(doc, tuple) and len(doc) > 1 and isinstance(doc[1], dict):
                        doc_content = doc[1].get("content", "")
                        if doc_content:
                            context_parts.append(doc_content)
            
            result = "\n\n".join(context_parts) if context_parts else ""
            logging.info(f"üîç DirectAnswer context length: {len(result)}")
            return result

        llm_with_tools = llm.bind_tools(tools)
        runnable = (
            RunnablePassthrough.assign(context=lambda ctx: get_combined_context(ctx))
            | prompt
            | llm_with_tools
        )
        super().__init__(runnable)
    
    def __call__(self, state: RagState, config) -> Dict[str, Any]:
        """Override to ensure context generation works with full state."""
        import logging
        from src.core.logging_config import log_exception_details
        
        # Log that we're using the simplified version
        logging.info("üî• USING SIMPLIFIED DirectAnswerAssistant")
        
        try:
            # Prepare prompt data
            prompt_data = self.binding_prompt(state)
            
            # Merge state with prompt_data
            full_state = {**state, **prompt_data}
            
            # Extract user_id for tool calls
            user_data = state.get("user", {})
            user_info = user_data.get("user_info", {})
            user_id = user_info.get("user_id", "unknown")
            logging.info(f"üîç DirectAnswer user_id: {user_id}")
            
            # Call runnable
            result = self.runnable.invoke(full_state, config)
            
            if self._is_valid_response(result):
                logging.info("‚úÖ DirectAnswerAssistant: Valid response generated.")
                return result
            else:
                logging.warning("‚ö†Ô∏è DirectAnswerAssistant: Invalid response, using fallback.")
                return self._create_fallback_response(state)
                
        except Exception as e:
            user_data = state.get("user", {})
            user_info = user_data.get("user_info", {"user_id": "unknown"})
            user_id = user_info.get("user_id", "unknown")
                
            logging.error(f"‚ùå DirectAnswerAssistant.__call__ - Exception: {type(e).__name__}: {str(e)}")
            log_exception_details(
                exception=e,
                context="DirectAnswerAssistant LLM call failed",
                user_id=user_id
            )
            
            return self._create_fallback_response(state)
