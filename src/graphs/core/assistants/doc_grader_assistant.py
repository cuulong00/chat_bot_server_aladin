from __future__ import annotations

from datetime import datetime
import logging
import traceback
from typing import Any

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableConfig
from pydantic import BaseModel, Field

from src.graphs.core.assistants.base_assistant import BaseAssistant
from src.graphs.state.state import RagState
from src.core.logging_config import log_exception_details


class GradeDocuments(BaseModel):
    """Pydantic model for the output of the document grader."""
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )


class DocGraderAssistant(BaseAssistant):
    """
    An assistant that grades the relevance of a document to the user's question.
    """
    def __init__(self, llm: Runnable, domain_context: str):
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - domain_context: {domain_context}")
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - llm type: {type(llm)}")
        
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                "system",
                "ğŸ” **Báº N LÃ€ CHUYÃŠN GIA ÄÃNH GIÃ Má»¨C Äá»˜ LIÃŠN QUAN Cá»¦A TÃ€I LIá»†U**\n\n"
                "**NHIá»†M Vá»¤ CHÃNH:** ÄÃ¡nh giÃ¡ xem tÃ i liá»‡u cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng hay khÃ´ng.\n\n"
                "**TIÃŠU CHÃ ÄÃNH GIÃ NGHIÃŠM NGáº¶T:**\n"
                "âœ… **TRáº¢ Lá»œI 'yes' KHI:**\n"
                "â€¢ TÃ i liá»‡u chá»©a thÃ´ng tin trá»±c tiáº¿p tráº£ lá»i cÃ¢u há»i\n"
                "â€¢ TÃ i liá»‡u Ä‘á» cáº­p Ä‘áº¿n cÃ¹ng chá»§ Ä‘á»/khÃ¡i niá»‡m chÃ­nh vá»›i cÃ¢u há»i\n"
                "â€¢ TÃ i liá»‡u cÃ³ tá»« khÃ³a quan trá»ng liÃªn quan Ä‘áº¿n cÃ¢u há»i\n"
                "â€¢ TÃ i liá»‡u cung cáº¥p bá»‘i cáº£nh há»¯u Ã­ch cho cuá»™c há»™i thoáº¡i\n\n"
                "âŒ **TRáº¢ Lá»œI 'no' KHI:**\n"
                "â€¢ TÃ i liá»‡u hoÃ n toÃ n khÃ´ng liÃªn quan Ä‘áº¿n cÃ¢u há»i\n"
                "â€¢ TÃ i liá»‡u chá»‰ cÃ³ sá»± trÃ¹ng láº·p tá»« ngáº«u nhiÃªn\n"
                "â€¢ TÃ i liá»‡u vá» chá»§ Ä‘á» khÃ¡c hoÃ n toÃ n\n"
                "â€¢ KhÃ´ng thá»ƒ tÃ¬m tháº¥y báº¥t ká»³ má»‘i liÃªn há»‡ nÃ o\n\n"
                "**NGUYÃŠN Táº®C ÄÃNH GIÃ CÃ”NG TÃ‚ME:**\n"
                "â€¢ ÄÃ¡nh giÃ¡ khÃ¡ch quan, khÃ´ng thiÃªn vá»‹\n"
                "â€¢ Æ¯u tiÃªn Ä‘á»™ chÃ­nh xÃ¡c hÆ¡n lÃ  tá»· lá»‡ pass/fail\n"
                "â€¢ Náº¿u cÃ³ nghi ngá» vÃ  tÃ i liá»‡u THá»°C Sá»° khÃ´ng liÃªn quan â†’ chá»n 'no'\n"
                "â€¢ Náº¿u cÃ³ má»‘i liÃªn há»‡ há»£p lÃ½ â†’ chá»n 'yes'\n\n"
                "**Bá»I Cáº¢NH HIá»†N Táº I:**\n"
                "â€¢ NgÃ y: {current_date}\n"
                "â€¢ Domain: {domain_context}\n"
                "â€¢ Cuá»™c há»™i thoáº¡i: {conversation_summary}\n\n"
                "**CHá»ˆ TRáº¢ Lá»œI:** 'yes' hoáº·c 'no'"
            ),
            ("human", 
             "**TÃ€I LIá»†U Cáº¦N ÄÃNH GIÃ:**\n{document}\n\n"
             "**CÃ‚U Há»I Cá»¦A NGÆ¯á»œI DÃ™NG:**\n{messages}\n\n"
             "**YÃŠU Cáº¦U:** ÄÃ¡nh giÃ¡ tÃ i liá»‡u cÃ³ liÃªn quan Ä‘áº¿n cÃ¢u há»i khÃ´ng? (yes/no)"
            ),
            ]
        ).partial(domain_context=domain_context, current_date=datetime.now())
        
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - prompt created with partial values")

        runnable = prompt | llm.with_structured_output(GradeDocuments)
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - runnable created with structured output")
        
        super().__init__(runnable)
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - completed")

    def __call__(self, state: RagState, config: RunnableConfig) -> GradeDocuments:
        """Override to add detailed logging for debugging."""
        
        try:
            # Call parent implementation and log every step
            logging.info(f"ğŸ” DocGraderAssistant.__call__ - calling super().__call__")
            
            # DETAILED LOGGING for DocGrader input analysis - BEFORE calling super()
            logging.info(f"ğŸ“‹ DOCGRADER PRE-EXECUTION INPUT ANALYSIS:")
            
            doc = state.get('document', 'MISSING')
            if doc and doc != 'MISSING':
                doc_content = doc.get('content', '') if isinstance(doc, dict) else str(doc)
                logging.info(f"   ğŸ“„ Document in state: {doc_content[:200]}...")
            else:
                logging.info(f"   ğŸ“„ Document in state: MISSING")
                
            logging.info(f"   â“ Messages in state: {state.get('messages', 'MISSING')}")
            logging.info(f"   ğŸ‘¤ User in state: {state.get('user', 'MISSING')}")
            logging.info(f"   ğŸ“Š All state keys: {list(state.keys())}")
            
            result = super().__call__(state, config)
            
            # Log the EXACT decision made by LLM
            logging.info(f"ğŸ¤– DOCGRADER FINAL DECISION ANALYSIS:")
            logging.info(f"   ğŸ“Š Result type: {type(result)}")
            logging.info(f"   âš–ï¸ Binary score: {getattr(result, 'binary_score', 'MISSING')}")
            if hasattr(result, 'binary_score'):
                logging.info(f"   ğŸ“ Decision summary: Document {'RELEVANT' if result.binary_score == 'yes' else 'NOT RELEVANT'}")
            logging.info(f"   ğŸ“„ Document that was evaluated: {state.get('document', 'MISSING')[:150] if state.get('document') else 'MISSING'}...")
            logging.info(f"   â“ Question that was evaluated: {state.get('messages', 'MISSING')}")
            
            logging.info(f"ğŸ” DocGraderAssistant.__call__ - super().__call__ returned type: {type(result)}")
            logging.info(f"ğŸ” DocGraderAssistant.__call__ - super().__call__ returned content: {result}")
            
            # Check if result has the expected binary_score attribute
            if hasattr(result, 'binary_score'):
                logging.info(f"âœ… DocGraderAssistant.__call__ - result has binary_score: {result.binary_score}")
            else:
                logging.error(f"âŒ DocGraderAssistant.__call__ - result missing binary_score attribute")
                logging.error(f"âŒ DocGraderAssistant.__call__ - result attributes: {dir(result) if result else 'None'}")
                
            return result
            
        except Exception as e:
            logging.error(f"âŒ DocGraderAssistant.__call__ - EXCEPTION occurred:")
            logging.error(f"   Exception type: {type(e).__name__}")
            logging.error(f"   Exception message: {str(e)}")
            logging.error(f"   Full traceback:\n{traceback.format_exc()}")
            
            # Re-raise to let parent handle
            raise e

    def _is_valid_response(self, result: Any) -> bool:
        """Override to properly validate GradeDocuments structured output."""
        logging.debug(f"ğŸ” DocGraderAssistant._is_valid_response - checking result type: {type(result)}")
        
        # For structured output (GradeDocuments), check if it has binary_score
        if isinstance(result, GradeDocuments):
            is_valid = hasattr(result, 'binary_score') and result.binary_score in ['yes', 'no']
            logging.debug(f"ğŸ” DocGraderAssistant._is_valid_response - GradeDocuments valid: {is_valid}, score: {getattr(result, 'binary_score', 'MISSING')}")
            return is_valid
        
        # Fall back to parent validation for other types
        parent_valid = super()._is_valid_response(result)
        logging.debug(f"ğŸ” DocGraderAssistant._is_valid_response - parent validation: {parent_valid}")
        return parent_valid
