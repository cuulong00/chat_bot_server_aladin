from __future__ import annotations

from datetime import datetime
import logging
import traceback
from typing import Any

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import Runnable, RunnableConfig
from pydantic import BaseModel, Field

from src.graphs.core.assistants.base_assistant import BaseAssistant
from src.graphs.state.state import RagState
from src.core.logging_config import log_exception_details


class GradeDocuments(BaseModel):
    """Pydantic model for the output of the document grader."""
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )


class DocGraderAssistant(BaseAssistant):
    """
    An assistant that grades the relevance of a document to the user's question.
    """
    def __init__(self, llm: Runnable, domain_context: str):
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - domain_context: {domain_context}")
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - llm type: {type(llm)}")
        
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                "system",
                # ROLE DEFINITION
                "You are a document relevance expert. Evaluate if documents are relevant to user questions.\n\n"
                
                # CORE EVALUATION CRITERIA
                "ğŸ“‹ RELEVANCE CRITERIA:\n"
                "âœ… RELEVANT (yes) when document contains:\n"
                "â€¢ Direct information answering the question\n"
                "â€¢ Related topics/concepts to the question\n"
                "â€¢ Important keywords matching the query\n"
                "â€¢ Useful context for the conversation\n\n"
                
                # DOMAIN-SPECIFIC RULES - PROMOTION PRIORITY
                "ğŸ¯ PROMOTION QUERY MATCHING (HIGHEST PRIORITY):\n"
                "**IF query contains ANY promotion keywords ('Æ°u Ä‘Ã£i', 'khuyáº¿n mÃ£i', 'chÆ°Æ¡ng trÃ¬nh', 'giáº£m giÃ¡', 'combo', 'táº·ng', 'promotion', 'discount'):**\n"
                "â€¢ Documents with 'Æ°u Ä‘Ã£i', 'khuyáº¿n mÃ£i', 'chÆ°Æ¡ng trÃ¬nh', 'combo', 'táº·ng', 'thÃ nh viÃªn', 'giáº£m' = **ALWAYS YES**\n"
                "â€¢ Documents mentioning prices, discounts, offers = **ALWAYS YES**\n"
                "â€¢ Menu documents (may contain combo/promotion info) = **YES**\n"
                "â€¢ Restaurant info documents (may mention offers) = **YES**\n"
                "â€¢ **FOR PROMOTION QUERIES: When in doubt â†’ YES**\n\n"
                
                "ğŸ¯ OTHER DOMAIN-SPECIFIC MATCHING:\n"
                "â€¢ Menu queries ('áº£nh menu', 'mÃ³n Äƒn', 'giÃ¡ cáº£') â†’ menu/combo/food docs = YES\n"
                "â€¢ Booking queries ('Ä‘áº·t bÃ n', 'ship') â†’ booking/delivery docs = YES\n"
                "â€¢ Delivery queries ('giao hÃ ng', 'ship mang vá»', 'delivery') â†’ shipping docs = YES\n"
                "â€¢ Branch queries ('chi nhÃ¡nh', 'cÆ¡ sá»Ÿ', 'Ä‘á»‹a chá»‰', location names) â†’ location docs = YES\n"
                "â€¢ Any location keywords (hÃ  ná»™i, tp.hcm, vincom, times city) â†’ branch info = YES\n"
                "â€¢ **Restaurant info, brand story, menu info â†’ potentially relevant for most restaurant queries = LEAN YES**\n\n"
                
                # DECISION RULES - AGGRESSIVE PROMOTION MATCHING
                "âš–ï¸ DECISION RULES:\n"
                "â€¢ **PROMOTION QUERIES: BIAS HEAVILY TOWARD YES** - Any restaurant document may contain relevant promotion info\n"
                "â€¢ **GENERAL QUERIES: BIAS TOWARD RELEVANCE** - When uncertain but potentially useful â†’ YES\n"
                "â€¢ Restaurant context: most restaurant docs can help answer restaurant questions\n"
                "â€¢ Only choose NO when document is completely off-topic (non-restaurant content)\n"
                "â€¢ Context understanding > strict keyword matching\n\n"
                
                # CURRENT CONTEXT
                "ğŸ“… Context: {current_date} | Domain: {domain_context} | Conversation: {conversation_summary}\n\n"
                
                "**OUTPUT:** 'yes' or 'no' only"
            ),
            ("human", 
             "**Document:** {document}\n"
             "**Question:** {messages}\n"
             "**Task:** Is document relevant to question? (yes/no)"
            ),
            ]
        ).partial(domain_context=domain_context, current_date=datetime.now())
        
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - prompt created with partial values")

        runnable = prompt | llm.with_structured_output(GradeDocuments)
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - runnable created with structured output")
        
        super().__init__(runnable)
        logging.info(f"ğŸ” DocGraderAssistant.__init__ - completed")

    def __call__(self, state: RagState, config: RunnableConfig) -> GradeDocuments:
        """Override to add detailed logging for debugging."""
        
        try:
            # Call parent implementation and log every step
            logging.info(f"ğŸ” DocGraderAssistant.__call__ - calling super().__call__")
            
            # DETAILED LOGGING for DocGrader input analysis - BEFORE calling super()
            logging.info(f"ğŸ“‹ DOCGRADER PRE-EXECUTION INPUT ANALYSIS:")
            
            doc = state.get('document', 'MISSING')
            if doc and doc != 'MISSING':
                doc_content = doc.get('content', '') if isinstance(doc, dict) else str(doc)
                logging.info(f"   ğŸ“„ Document in state: {doc_content[:200]}...")
            else:
                logging.info(f"   ğŸ“„ Document in state: MISSING")
                
            logging.info(f"   â“ Messages in state: {state.get('messages', 'MISSING')}")
            logging.info(f"   ğŸ‘¤ User in state: {state.get('user', 'MISSING')}")
            logging.info(f"   ğŸ“Š All state keys: {list(state.keys())}")
            
            result = super().__call__(state, config)
            
            # Log the EXACT decision made by LLM
            logging.info(f"ğŸ¤– DOCGRADER FINAL DECISION ANALYSIS:")
            logging.info(f"   ğŸ“Š Result type: {type(result)}")
            logging.info(f"   âš–ï¸ Binary score: {getattr(result, 'binary_score', 'MISSING')}")
            if hasattr(result, 'binary_score'):
                logging.info(f"   ğŸ“ Decision summary: Document {'RELEVANT' if result.binary_score == 'yes' else 'NOT RELEVANT'}")
            logging.info(f"   ğŸ“„ Document that was evaluated: {state.get('document', 'MISSING')[:150] if state.get('document') else 'MISSING'}...")
            logging.info(f"   â“ Question that was evaluated: {state.get('messages', 'MISSING')}")
            
            logging.info(f"ğŸ” DocGraderAssistant.__call__ - super().__call__ returned type: {type(result)}")
            logging.info(f"ğŸ” DocGraderAssistant.__call__ - super().__call__ returned content: {result}")
            
            # Check if result has the expected binary_score attribute
            if hasattr(result, 'binary_score'):
                logging.info(f"âœ… DocGraderAssistant.__call__ - result has binary_score: {result.binary_score}")
            else:
                logging.error(f"âŒ DocGraderAssistant.__call__ - result missing binary_score attribute")
                logging.error(f"âŒ DocGraderAssistant.__call__ - result attributes: {dir(result) if result else 'None'}")
                
            return result
            
        except Exception as e:
            logging.error(f"âŒ DocGraderAssistant.__call__ - EXCEPTION occurred:")
            logging.error(f"   Exception type: {type(e).__name__}")
            logging.error(f"   Exception message: {str(e)}")
            logging.error(f"   Full traceback:\n{traceback.format_exc()}")
            
            # Re-raise to let parent handle
            raise e

    def _is_valid_response(self, result: Any) -> bool:
        """Override to properly validate GradeDocuments structured output."""
        logging.debug(f"ğŸ” DocGraderAssistant._is_valid_response - checking result type: {type(result)}")
        
        # For structured output (GradeDocuments), check if it has binary_score
        if isinstance(result, GradeDocuments):
            is_valid = hasattr(result, 'binary_score') and result.binary_score in ['yes', 'no']
            logging.debug(f"ğŸ” DocGraderAssistant._is_valid_response - GradeDocuments valid: {is_valid}, score: {getattr(result, 'binary_score', 'MISSING')}")
            return is_valid
        
        # Fall back to parent validation for other types
        parent_valid = super()._is_valid_response(result)
        logging.debug(f"ğŸ” DocGraderAssistant._is_valid_response - parent validation: {parent_valid}")
        return parent_valid
