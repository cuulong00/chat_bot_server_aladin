import logging
import traceback
import copy
import time
import os
from pathlib import Path

from typing import List, TypedDict, Annotated, Literal

from datetime import datetime

# Import centralized logging configuration
from src.core.logging_config import (
    setup_advanced_logging,
    log_exception_details,
    get_logger,
    log_business_event,
    log_performance_metric
)

from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import  ToolNode
from langgraph.graph import StateGraph

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import Runnable, RunnableConfig, RunnablePassthrough

from src.utils.query_classifier import QueryClassifier

from src.tools.memory_tools import save_user_preference, get_user_profile
from src.tools.image_analysis_tool import analyze_image
from src.services.image_processing_service import get_image_processing_service
from src.graphs.state.state import RagState
from src.database.qdrant_store import QdrantStore
"""Adaptive RAG graph with optional short-term memory (langmem).

If langmem is not installed or incompatible with the installed langgraph version,
we fall back to a lightweight no-op summarization node so the app still starts.
"""

# Optional langmem import (may fail if version mismatch with langgraph)
try:  # pragma: no cover - defensive import
    from langmem.short_term import SummarizationNode, RunningSummary  # type: ignore
    _LANGMEM_AVAILABLE = True
except Exception as _langmem_err:  # noqa: BLE001
    _LANGMEM_AVAILABLE = False
    logging.warning(
        "LangMem unavailable (%s). Running without short-term summarization."
        " Install a compatible 'langmem' & 'langgraph' to enable it.",
        _langmem_err,
    )

    class RunningSummary:  # minimal stub
        def __init__(self, max_tokens: int = 1200):
            self.max_tokens = max_tokens
            self.summary = ""  # kept for attribute compatibility

        def append(self, _text: str):  # no-op
            return None

    class SummarizationNode:  # stub that returns empty update
        def __init__(self, *_, **__):
            pass

        def __call__(self, state: RagState, config: RunnableConfig):  # returns nothing so graph continues
            return {}
from langchain_core.messages.utils import count_tokens_approximately
from operator import itemgetter
from langchain_tavily import TavilySearch
from src.nodes.nodes import *

# Get logger for this module
logger = get_logger(__name__)


# --- State Reset and Management Functions ---
def get_current_user_question(state: RagState) -> str:
    """
    Consistently get the current user question from state.
    This function ensures all nodes use the same logic to extract the current question.
    """
    question = state.get("question", "")
    if not question:
        # Fallback to last human message
        messages = state.get("messages", [])
        if messages:
            for msg in reversed(messages):
                if isinstance(msg, HumanMessage):
                    question = extract_text_from_message_content(msg.content)
                    break
        else:
            # N·∫øu kh√¥ng c√≥ messages, ki·ªÉm tra input
            input_data = state.get("input", {})
            if isinstance(input_data, dict) and "messages" in input_data:
                input_messages = input_data["messages"]
                for msg in reversed(input_messages):
                    if isinstance(msg, dict) and msg.get("type") == "human":
                        question = msg.get("content", "")
                        break
    
    # Fallback cu·ªëi c√πng
    if not question:
        question = "C√¢u h·ªèi kh√¥ng r√µ r√†ng"
        logging.warning("No valid question found in state, using fallback")
    
    return question.strip() if question else "C√¢u h·ªèi kh√¥ng r√µ r√†ng"


def reset_state_for_new_query(state: RagState) -> dict:
    """
    Reset state for a completely new user query.
    This ensures clean state for new queries.
    
    Args:
        state: Current RAG state
        
    Returns:
        dict: State update with reset state
    """
    # Get current user question to compare
    current_question = get_current_user_question(state)
    
    # Reset state for new query to prevent accumulation
    logging.info(f"üßπ Resetting for new query: {current_question[:50]}{'...' if len(current_question) > 50 else ''}")
    
    return {
        "question": current_question,  # Ensure question is set in state
    }


def should_reset_dialog_state(state: RagState) -> bool:
    """
    Determine if dialog_state should be reset based on conversation flow.
    Reset when starting a completely new conversation topic.
    """
    messages = state.get("messages", [])
    
    # Reset if this is the first message or very few messages
    if len(messages) <= 2:
        return True
        
    # Check if this is a new conversation topic (simple heuristic)
    current_question = get_current_user_question(state)
    if current_question and any(greeting in current_question.lower() for greeting in ["xin ch√†o", "hello", "hi", "ch√†o"]):
        return True
        
    return False


def should_summarize_conversation(state: RagState) -> bool:
    """
    FIXED: Determine if conversation should be summarized based on message count and token count.
    Only summarize when conversation gets long to prevent summary content replacing AI responses.
    """
    messages = state.get("messages", [])
    
    # Don't summarize for short conversations (avoid summary replacing actual responses)
    if len(messages) <= 8:  # Increased threshold - only summarize longer conversations
        return False
        
    # Check token count to determine if summarization is needed
    total_tokens = 0
    for message in messages:
        content = message.content if hasattr(message, 'content') else str(message)
        if isinstance(content, str):
            total_tokens += len(content.split()) * 1.3  # Rough token estimation
        elif isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and 'text' in item:
                    total_tokens += len(str(item['text']).split()) * 1.3
    
    # Only summarize if conversation is getting long (more than ~2000 tokens)
    should_summarize = total_tokens > 2500
    
    logging.info(f"üß† Summarization decision: messages={len(messages)}, tokens‚âà{int(total_tokens)}, summarize={should_summarize}")
    
    return should_summarize





def truncate_for_logging(data, max_len=250):
    data_copy = copy.deepcopy(data)
    if isinstance(data_copy, str):
        if len(data_copy) > max_len:
            return data_copy[:max_len] + "..."
        return data_copy
    elif isinstance(data_copy, dict):
        if "embedding" in data_copy and isinstance(data_copy["embedding"], list):
            data_copy["embedding"] = f"[<{len(data_copy['embedding'])} numbers vector>]"
        for k, v in data_copy.items():
            if k != "embedding":
                data_copy[k] = truncate_for_logging(v, max_len)
        return data_copy
    elif isinstance(data_copy, list):
        return [truncate_for_logging(item, max_len) for item in data_copy]
    elif isinstance(data_copy, tuple):
        return tuple(truncate_for_logging(item, max_len) for item in data_copy)
    else:
        return data_copy


def get_message_content(msg: BaseMessage) -> str:
    """Extract text content from a message using the helper function."""
    return extract_text_from_message_content(msg.content)


def get_last_user_question(messages: List[BaseMessage]) -> HumanMessage | None:
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            return msg
    return None


def extract_text_from_message_content(content) -> str:
    """Extract text from message content, handling both string and list formats."""
    if isinstance(content, str):
        return content.strip()
    elif isinstance(content, list):
        text = " ".join(
            item.get("text", "")
            for item in content
            if isinstance(item, dict) and "text" in item
        )
        return text.strip()
    else:
        return str(content).strip()


# --- State Definition ---


def get_question_from_state(state: RagState) -> str:
    """Extract question from state, with fallback to last message content."""
    # Use the consistent utility function
    return get_current_user_question(state)


# --- Pydantic Models for Structured Output ---
class RouteQuery(BaseModel):
    datasource: Literal["vectorstore", "web_search", "direct_answer", "process_document"] = Field(
        ...,
        description="Given a user question, choose to route it to web search, a vectorstore, to answer directly, or to process documents/images.",
    )


class GradeDocuments(BaseModel):
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )


class GradeHallucinations(BaseModel):
    binary_score: str = Field(
        description="Answer is grounded in the facts AND answers the user's question, 'yes' or 'no'"
    )


# --- Assistant Class Pattern ---
class Assistant:
    def __init__(self, runnable: Runnable):
        self.runnable = runnable

    def binding_prompt(self, state: RagState):
        # L·∫•y summary context t·ª´ state
        running_summary = ""
        if state.get("context") and isinstance(state["context"], dict):
            summary_obj = state["context"].get("running_summary")
            if summary_obj and hasattr(summary_obj, "summary"):
                running_summary = summary_obj.summary
                logging.debug(f"running_summary:{running_summary}")

        # Ki·ªÉm tra v√† x·ª≠ l√Ω user data an to√†n
        user_data = state.get("user", {})
        if not user_data:
            logging.warning("No user data found in state, using defaults")
            user_data = {
                "user_info": {"user_id": "unknown"},
                "user_profile": {}
            }
        
        user_info = user_data.get("user_info", {"user_id": "unknown"})
        user_profile = user_data.get("user_profile", {})

        prompt = {
            **state,
            "user_info": user_info,
            "user_profile": user_profile,
            "conversation_summary": running_summary,
        }
        
        # Validate that essential fields exist
        if not prompt.get("messages"):
            logging.error("No messages found in prompt data")
            prompt["messages"] = "C√¢u h·ªèi kh√¥ng r√µ r√†ng"
        
        return prompt

    def __call__(self, state: RagState, config: RunnableConfig):
        """
        Execute the assistant with improved retry mechanism and fallback handling.
        
        Handles empty LLM responses by retrying with exponential backoff and providing
        graceful fallback when max retries are reached.
        """
        max_retries = 0
        retry_count = 0
        base_delay = 0.5  # Base delay in seconds
        
        #while retry_count <= max_retries:
        try:
            # Configure user ID for request context - v·ªõi x·ª≠ l√Ω an to√†n
            user_data = state.get("user", {})
            user_info = user_data.get("user_info", {}) if user_data else {}
            user_id = user_info.get("user_id", "unknown") if user_info else "unknown"
            
            if "configurable" not in config:
                config["configurable"] = {}
            config["configurable"]["user_id"] = user_id
            
            # Prepare prompt data for LLM
            prompt = self.binding_prompt(state)
            
            # Validate prompt before sending to LLM
            if not prompt or not prompt.get("messages"):
                logging.error("Empty or invalid prompt data")
                raise ValueError("Prompt data is empty or missing required fields")
            
            # Invoke LLM with current configuration
            result = self.runnable.invoke(prompt, config)
            
            # For structured output (Pydantic model), return immediately
            if not hasattr(result, "tool_calls"):
                logging.debug(f"Assistant: Structured output returned after {retry_count} retries")
                return result
            
            # Validate message content for completeness
            if self._is_valid_response(result):
                if retry_count > 0:
                    logging.info(f"Assistant: Valid response received after {retry_count} retries")
                return result
            
            # Handle retry logic for invalid responses
            retry_count += 1
            if retry_count <= max_retries:
                delay = base_delay * (2 ** (retry_count - 1))  # Exponential backoff
                logging.warning(
                    f"Assistant: Empty/invalid response from LLM, retrying... "
                    f"({retry_count}/{max_retries}) - waiting {delay}s"
                )
                
                # Add small delay to prevent rapid successive calls
                time.sleep(delay)
            else:
                logging.error(f"Assistant: Max retries ({max_retries}) reached, providing fallback response")
                return self._create_fallback_response(state)
                
        except Exception as e:
            retry_count += 1
            user_id = state.get("user", {}).get("user_info", {}).get("user_id", "unknown")
            
            # Ki·ªÉm tra l·ªói c·ª• th·ªÉ c·ªßa Gemini
            if "contents is not specified" in str(e):
                logging.error(f"Gemini API error - empty contents detected. Prompt validation failed.")
                logging.debug(f"State keys: {list(state.keys()) if state else 'None'}")
                logging.debug(f"Messages in state: {state.get('messages', 'None')}")
            
            # Log detailed exception information to file
            log_exception_details(
                exception=e,
                context=f"Assistant LLM call failure (attempt {retry_count})",
                user_id=user_id
            )
            
            if retry_count <= max_retries:
                delay = base_delay * (2 ** (retry_count - 1))
                logging.warning(f"Assistant: Retrying after exception - waiting {delay}s")
                time.sleep(delay)
            else:
                logging.error(f"Assistant: Max retries reached after exceptions, providing fallback")
                return self._create_fallback_response(state)
        
        # Should never reach here, but provide fallback just in case
        return self._create_fallback_response(state)

    def _is_valid_response(self, result) -> bool:
        """
        Validate if the LLM response contains meaningful content.
        
        Args:
            result: The result from LLM invocation
            
        Returns:
            bool: True if response is valid, False otherwise
        """
        # Check for tool calls - these are always valid
        if hasattr(result, "tool_calls") and result.tool_calls:
            return True
        
        # Check content validity
        content = getattr(result, "content", None)
        
        if not content:
            return False
        
        # Handle string content
        if isinstance(content, str):
            return content.strip() != ""
        
        # Handle list content (multimodal messages)
        if isinstance(content, list):
            for item in content:
                if isinstance(item, dict) and item.get("text", "").strip():
                    return True
            return False
        
        # For other content types, consider non-empty as valid
        return bool(content)

    def _create_fallback_response(self, state: RagState):
        """
        Create a graceful fallback response when LLM fails after retries.
        
        Args:
            state: Current conversation state
            
        Returns:
            AIMessage: Fallback response message
        """
        # Get user info for personalized fallback
        user_info = state.get("user", {}).get("user_info", {})
        user_name = user_info.get("name", "anh/ch·ªã")
        
        # Create contextual fallback message
        fallback_content = (
            f"Xin l·ªói {user_name}, em ƒëang g·∫∑p v·∫•n ƒë·ªÅ k·ªπ thu·∫≠t t·∫°m th·ªùi. "
            f"Vui l√≤ng th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá hotline 1900 636 886 ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ tr·ª±c ti·∫øp. "
            f"Em r·∫•t xin l·ªói v√¨ s·ª± b·∫•t ti·ªán n√†y! üôè"
        )
        
        logging.info(f"Assistant: Providing fallback response for user: {user_info.get('user_id', 'unknown')}")
        
        return AIMessage(
            content=fallback_content,
            additional_kwargs={"fallback_response": True}
        )


# --- Graph Definition Function ---
def create_adaptive_rag_graph(
    llm: Runnable,
    llm_grade_documents: Runnable,
    llm_router: Runnable,
    llm_rewrite: Runnable,
    llm_generate_direct: Runnable,
    llm_hallucination_grader: Runnable,
    llm_summarizer: Runnable,
    llm_contextualize: Runnable,
    retriever: QdrantStore,
    tools: list,
    DOMAIN: dict,
):

    # --- Bind domain config ---
    domain_context = DOMAIN["domain_context"]
    domain_instructions = DOMAIN["domain_instructions"]
    domain_examples = "\n".join(DOMAIN["domain_examples"])

    web_search_tool = TavilySearch(max_results=5)
    memory_tools = [get_user_profile, save_user_preference]
    image_tools = [analyze_image]
    all_tools = tools + [web_search_tool] + memory_tools + image_tools

    # === Chains for Summarization and Contextualization ===

    summarizer_prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="messages"),
            (
                "user",
                "Concisely summarize the conversation above. Extract key information, user preferences, and decisions. The summary will be used as context for the next turn. Keep the summary in the original language of the conversation.",
            ),
        ]
    )
    summarizer_chain = summarizer_prompt | llm_summarizer

    contextualize_q_system_prompt = """Given a chat history (which may include a summary of earlier messages) and the latest user question, \
formulate a standalone question which can be understood without the chat history. Do NOT answer the question, \
just reformulate it if needed and otherwise return it as is. Keep the question in its original language."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder(variable_name="messages"),
            ("human", "Rephrase the last user question to be a standalone question."),
        ]
    )
    contextualize_q_chain = contextualize_q_prompt | llm_contextualize

    # === Assistants and Runnables (Restored with full context) ===

    # 1. Router Assistant
    router_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "Current date for context is: {current_date}\n"
                "You are a highly efficient routing agent about {domain_context}. Your ONLY job: return exactly one token from this set: vectorstore | web_search | direct_answer | process_document.\n\n"
                "DECISION ALGORITHM (execute in order, stop at first match):\n"
                "1. PROCESS_DOCUMENT (DOCUMENT/IMAGE ANALYSIS) -> Choose 'process_document' if the user:\n"
                "   - **HIGHEST PRIORITY: Message contains attachment metadata like '[H√åNH ·∫¢NH] URL:', '[VIDEO] URL:', '[T·ªÜP TIN] URL:' - ALWAYS route to process_document**, OR\n"
                "   - Message contains 'üì∏ **Ph√¢n t√≠ch h√¨nh ·∫£nh:**' (pre-analyzed content), OR\n"
                "   - Sends or mentions documents, files, attachments, images that need analysis (mentions of 'h√¨nh ·∫£nh', '·∫£nh', 'photo', 'image', 'xem ƒë∆∞·ª£c', 'trong h√¨nh', 'giao di·ªán', 't√†i li·ªáu', 'file', 'ƒë√≠nh k√®m'), OR\n"
                "   - Asks about content in images, photos, documents they have sent, OR\n"
                "   - Questions that reference visual or document content that requires analysis tools rather than knowledge retrieval, OR\n"
                "   - Requests analysis or description of attached media content.\n"
                "   Rationale: these require specialized document/image processing capabilities and tools.\n"
                "2. DIRECT_ANSWER (ACTION/CONFIRMATION/SMALL TALK) -> Choose 'direct_answer' if the user is:\n"
                "   - Giving confirmation/negation or supplying details in an ongoing flow (e.g., 'kh√¥ng c√≥ ai sinh nh·∫≠t', '7h t·ªëi nay', '3 ng∆∞·ªùi l·ªõn 2 tr·∫ª em'), OR\n"
                "   - Expressing intent to perform an action like booking ('ƒë·∫∑t b√†n', 'ƒë·∫∑t ch·ªó', 'book', 'booking', 'gi·ªØ b√†n'), OR\n"
                "   - Greeting/thanks/chit‚Äëchat/meta about the assistant, OR\n"
                "   - Asking about or updating personal preferences.\n"
                "   Rationale: these do not require knowledge retrieval; they should be handled by conversational logic and tools.\n"
                "3. VECTORSTORE -> Choose 'vectorstore' only if the user asks for information that should come from internal knowledge (menu, ƒë·ªãa ch·ªâ, chi nh√°nh, hotline, ch√≠nh s√°ch, ∆∞u ƒë√£i, FAQ‚Ä¶) and is NOT merely confirming/continuing an action or sending documents.\n"
                "4. WEB_SEARCH -> Only if none of (1), (2), or (3) apply AND the user clearly needs real‚Äëtime external info.\n\n"
                "IMPORTANT: \n"
                "- If message contains document/image content or analysis ‚Üí ALWAYS choose 'process_document'\n"
                "- If both direct_answer and vectorstore could apply, prefer 'direct_answer' when the user is clearly in a booking or confirmation step\n"
                "- Document/image content takes highest priority over other routing decisions\n\n"
                "CONVERSATION CONTEXT SUMMARY (may strengthen decision toward vectorstore):\n{conversation_summary}\n\n"
                "User info:\n<UserInfo>\n{user_info}\n</UserInfo>\n"
                "User profile:\n<UserProfile>\n{user_profile}\n</UserProfile>\n\n"
                "Domain instructions (reinforce vectorstore bias):\n{domain_instructions}\n\n"
                "Return ONLY one of: vectorstore | web_search | direct_answer | process_document. No explanations.\n",
            ),
            ("human", "{messages}"),
        ]
    ).partial(
        current_date=datetime.now,
        domain_context=domain_context,
        domain_instructions=domain_instructions,
    )
    router_runnable = router_prompt | llm_router.with_structured_output(RouteQuery)
    router_assistant = Assistant(router_runnable)

    # 2. Document Grader Assistant
    doc_grader_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are an expert at evaluating if a document is topically relevant to a user's question.\n"
                "Your task is to determine if the document discusses the same general topic as the question, even if it doesn't contain the exact answer.\n"
                "Current date for context is: {current_date}\n"
                "Domain context: {domain_context}\n\n"
                # **TH√äM SUMMARY CONTEXT**
                "--- CONVERSATION CONTEXT ---\n"
                "Previous conversation summary:\n{conversation_summary}\n"
                "Use this context to better understand what the user is asking about and whether the document is relevant to the ongoing conversation.\n\n"
                "RELEVANCE BOOST FOR MENU QUERIES: If the user asks about 'menu', 'th·ª±c ƒë∆°n', 'm√≥n', 'gi√°', 'combo', 'set menu' then any document containing menu signals is relevant.\n"
                "Menu signals include words like 'TH·ª∞C ƒê∆†N', 'TH·ª∞C ƒê∆†N TI√äU BI·ªÇU', 'Combo', 'L·∫©u', lines with prices (c√≥ 'ƒë' ho·∫∑c 'k'), or explicit menu links (e.g., 'menu.tianlong.vn').\n"
                "RELEVANCE BOOST FOR ADDRESS QUERIES: If the user asks about 'ƒë·ªãa ch·ªâ', '·ªü ƒë√¢u', 'chi nh√°nh', 'branch', 'hotline', documents listing addresses, branches, cities, or hotline numbers are relevant. Lines that start with branch names or include street names/cities should be considered relevant.\n"
                "RELEVANCE BOOST FOR PROMOTION/DISCOUNT QUERIES: If the user asks about '∆∞u ƒë√£i', 'khuy·∫øn m√£i', 'gi·∫£m gi√°', 'ch∆∞∆°ng tr√¨nh', 'th√†nh vi√™n', 'discount', 'promotion', 'offer', 'program' then any document containing promotion signals is relevant.\n"
                "Promotion signals include words like '∆∞u ƒë√£i', 'khuy·∫øn m√£i', 'gi·∫£m', '%', 'th√†nh vi√™n', 'th·∫ª', 'B·∫†C', 'V√ÄNG', 'KIM C∆Ø∆†NG', 'sinh nh·∫≠t', 'Ng√†y h·ªôi', 'ch∆∞∆°ng tr√¨nh', or membership-related content.\n"
                "Does the document mention keywords or topics related to the user's question or the conversation context? "
                "For example, if the question is about today's date, any document discussing calendars, dates, or 'today' is relevant.\n"
                "Consider both the current question AND the conversation history when determining relevance.\n"
                "Respond with only 'yes' or 'no'.",
            ),
            ("human", "Document:\n\n{document}\n\nQuestion: {messages}"),
        ]
    ).partial(domain_context=domain_context, current_date=datetime.now())

    doc_grader_runnable = (
        doc_grader_prompt | llm_grade_documents.with_structured_output(GradeDocuments)
    )
    doc_grader_assistant = Assistant(doc_grader_runnable)

    # 3. Rewrite Assistant
    # IMPORTANT: Include a human message so Gemini receives non-empty 'contents'.
    # Also use the normalized 'question' string instead of raw 'messages' list.
    rewrite_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a domain expert at rewriting user questions to optimize for document retrieval.\n"
                "Domain context: {domain_context}\n"
                "--- CONVERSATION CONTEXT ---\n"
                "Previous conversation summary:\n{conversation_summary}\n"
                "Use this context to understand what has been discussed before and rewrite the question to include relevant context that will help with document retrieval.\n\n"
                "Rewrite this question to be more specific and include relevant context from the conversation history that would help find better documents. "
                "If the question is about menu/dishes/prices, append helpful retrieval keywords such as 'TH·ª∞C ƒê∆†N', 'Combo', 'gi√°', 'set menu', 'Lo·∫°i l·∫©u', and 'Tian Long'. "
                "If the question is about locations/addresses/branches/hotline, append keywords such as 'ƒë·ªãa ch·ªâ', 'chi nh√°nh', 'branch', 'Hotline', 'H√† N·ªôi', 'H·∫£i Ph√≤ng', 'TP. H·ªì Ch√≠ Minh', 'Times City', 'Vincom', 'L√™ VƒÉn S·ªπ', and 'Tian Long'. "
                "If the question is about promotions/discounts/offers/membership, append keywords such as '∆∞u ƒë√£i', 'khuy·∫øn m√£i', 'gi·∫£m gi√°', 'ch∆∞∆°ng tr√¨nh th√†nh vi√™n', 'th·∫ª th√†nh vi√™n', 'B·∫†C', 'V√ÄNG', 'KIM C∆Ø∆†NG', 'sinh nh·∫≠t', 'Ng√†y h·ªôi th√†nh vi√™n', 'gi·∫£m %', and 'Tian Long'. "
                "Make sure the rewritten question is clear and contains keywords that would match relevant documents.",
            ),
            (
                "human",
                "Original question: {question}\n"
                "Please rewrite it as a concise, retrieval-friendly query in the SAME language.",
            ),
        ]
    ).partial(domain_context=domain_context)
    rewrite_runnable = rewrite_prompt | llm_rewrite
    rewrite_assistant = Assistant(rewrite_runnable)

    # 4. Main Generation Assistant
    generation_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "B·∫°n l√† Vy ‚Äì tr·ª£ l√Ω ·∫£o th√¢n thi·ªán v√† chuy√™n nghi·ªáp c·ªßa nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long (domain context: {domain_context}). "
                "B·∫°n lu√¥n ∆∞u ti√™n th√¥ng tin t·ª´ t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p (context) v√† cu·ªôc tr√≤ chuy·ªán. N·∫øu t√†i li·ªáu xung ƒë·ªôt v·ªõi ki·∫øn th·ª©c tr∆∞·ªõc ƒë√≥, B·∫†N PH·∫¢I tin t∆∞·ªüng t√†i li·ªáu.\n"
                "\n"
                "üéØ **VAI TR√í V√Ä PHONG C√ÅCH GIAO TI·∫æP:**\n"
                "- B·∫°n l√† nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng chuy√™n nghi·ªáp, l·ªãch s·ª± v√† nhi·ªát t√¨nh\n"
                "- **LOGIC CH√ÄO H·ªéI TH√îNG MINH:**\n"
                "  ‚Ä¢ **L·∫ßn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i:** Ch√†o h·ªèi ƒë·∫ßy ƒë·ªß v·ªõi t√™n kh√°ch h√†ng (n·∫øu c√≥) + gi·ªõi thi·ªáu nh√† h√†ng\n"
                "    V√≠ d·ª•: 'Ch√†o anh Tu·∫•n D∆∞∆°ng! Nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long hi·ªán c√≥ t·ªïng c·ªông 8 chi nh√°nh...'\n"
                "  ‚Ä¢ **T·ª´ c√¢u th·ª© 2 tr·ªü ƒëi:** Ch·ªâ c·∫ßn l·ªùi ch√†o ng·∫Øn g·ªçn l·ªãch s·ª±\n"
                "    V√≠ d·ª•: 'D·∫° anh/ch·ªã', 'D·∫° ·∫°', 'V√¢ng ·∫°'\n"
                "- S·ª≠ d·ª•ng ng√¥n t·ª´ t√¥n tr·ªçng: 'anh/ch·ªã', 'd·∫°', '·∫°', 'em Vy'\n"
                "- Th·ªÉ hi·ªán s·ª± quan t√¢m ch√¢n th√†nh ƒë·∫øn nhu c·∫ßu c·ªßa kh√°ch h√†ng\n"
                "- Lu√¥n k·∫øt th√∫c b·∫±ng c√¢u h·ªèi th√¢n thi·ªán ƒë·ªÉ ti·∫øp t·ª•c h·ªó tr·ª£\n"
                "\n"
                "üí¨ **ƒê·ªäNH D·∫†NG T·ªêI ∆ØU CHO FACEBOOK MESSENGER (R·∫§T QUAN TR·ªåNG):**\n"
                "- Messenger KH√îNG h·ªó tr·ª£ markdown/HTML ho·∫∑c b·∫£ng. Tr√°nh d√πng b·∫£ng '|' v√† k√Ω t·ª± k·∫ª d√≤ng '---'.\n"
                "- Tr√¨nh b√†y b·∫±ng c√°c ti√™u ƒë·ªÅ ng·∫Øn c√≥ emoji + danh s√°ch g·∫°ch ƒë·∫ßu d√≤ng. M·ªói d√≤ng ng·∫Øn, r√µ, 1 √Ω.\n"
                "- D√πng c·∫•u tr√∫c:\n"
                "  ‚Ä¢ Ti√™u ƒë·ªÅ khu v·ª±c (c√≥ emoji)\n"
                "  ‚Ä¢ C√°c m·ª•c con theo d·∫°ng bullet: '‚Ä¢ T√™n m√≥n ‚Äî Gi√° ‚Äî Ghi ch√∫' (d√πng d·∫•u '‚Äî' ho·∫∑c '-' ƒë·ªÉ ph√¢n t√°ch)\n"
                "- Gi·ªõi h·∫°n ƒë·ªô d√†i: t·ªëi ƒëa ~10 m·ª•c/m·ªôt danh s√°ch; n·∫øu nhi·ªÅu h∆°n, ghi 'Xem ƒë·∫ßy ƒë·ªß: <link>'.\n"
                "- D√πng kho·∫£ng tr·∫Øng d√≤ng ƒë·ªÉ t√°ch kh·ªëi n·ªôi dung. Tr√°nh d√≤ng qu√° d√†i.\n"
                "- ∆Øu ti√™n th√™m link ch√≠nh th·ª©c (menu/website) ·ªü cu·ªëi n·ªôi dung.\n"
                "- V√≠ d·ª• hi·ªÉn th·ªã menu ƒë·∫πp m·∫Øt:\n"
                "  üç≤ Th·ª±c ƒë∆°n ti√™u bi·ªÉu\n"
                "  ‚Ä¢ L·∫©u cay Tian Long ‚Äî 441.000ƒë ‚Äî D√†nh cho 2 kh√°ch\n"
                "  ‚Ä¢ COMBO TAM GIAO ‚Äî 555.000ƒë ‚Äî Ph√π h·ª£p 2-3 kh√°ch\n"
                "  ...\n"
                "  Xem ƒë·∫ßy ƒë·ªß menu: https://menu.tianlong.vn/\n"
                "\n"
                "üìã **X·ª¨ L√ù C√ÅC LO·∫†I C√ÇU H·ªéI:**\n"
                "\n"
                "**1Ô∏è‚É£ C√ÇU H·ªéI V·ªÄ TH·ª∞C ƒê∆†N/M√ìN ƒÇN:**\n"
                "Khi kh√°ch h·ªèi v·ªÅ menu, th·ª±c ƒë∆°n, m√≥n ƒÉn, gi√° c·∫£:\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- TUY·ªÜT ƒê·ªêI kh√¥ng d√πng b·∫£ng. H√£y tr√¨nh b√†y d·∫°ng danh s√°ch bullet theo nh√≥m: 'Lo·∫°i l·∫©u', 'Combo', 'M√≥n n·ªïi b·∫≠t'.\n"
                "- M·ªói d√≤ng: '‚Ä¢ T√™n m√≥n ‚Äî Gi√° ‚Äî Ghi ch√∫ (n·∫øu c√≥)'\n"
                "- T·ªëi ƒëa ~8‚Äì10 d√≤ng m·ªói nh√≥m; n·∫øu d√†i, g·ª£i √Ω link 'Xem ƒë·∫ßy ƒë·ªß menu: <link>'\n"
                "- D√πng emoji ph√¢n nh√≥m (üç≤, üßÜ, üßÄ, ü•©, ü•¨, ‚≠ê) v√† gi·ªØ b·ªë c·ª•c tho√°ng, d·ªÖ ƒë·ªçc\n"
                "- Cu·ªëi ph·∫ßn menu, lu√¥n ƒë√≠nh k√®m link menu ch√≠nh th·ª©c n·∫øu c√≥\n"
                "- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi h·ªó tr·ª£ th√™m\n"
                "\n"
                "**2Ô∏è‚É£ C√ÇU H·ªéI V·ªÄ ƒê·ªäA CH·ªà/CHI NH√ÅNH:**\n"
                "Khi kh√°ch h·ªèi v·ªÅ ƒë·ªãa ch·ªâ, chi nh√°nh:\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß + gi·ªõi thi·ªáu t·ªïng s·ªë chi nh√°nh; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- Tr√¨nh b√†y d·∫°ng m·ª•c l·ª•c ng·∫Øn g·ªçn, kh√¥ng b·∫£ng/markdown:\n"
                "  ‚Ä¢ Nh√≥m theo v√πng/city v·ªõi ti√™u ƒë·ªÅ c√≥ emoji (üìç H√† N·ªôi, üìç TP.HCM, ‚Ä¶)\n"
                "  ‚Ä¢ M·ªói d√≤ng 1 chi nh√°nh: '‚Ä¢ T√™n chi nh√°nh ‚Äî ƒê·ªãa ch·ªâ' (ng·∫Øn g·ªçn)\n"
                "  ‚Ä¢ N·∫øu c√≥ hotline/chung: th√™m ·ªü cu·ªëi ph·∫ßn '‚òéÔ∏è Hotline: 1900 636 886'\n"
                "- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi v·ªÅ nhu c·∫ßu ƒë·∫∑t b√†n\n"
                "\n"
                "**3Ô∏è‚É£ C√ÇU H·ªéI V·ªÄ ∆ØU ƒê√ÉI/KHUY·∫æN M√ÉI:**\n"
                "Khi kh√°ch h·ªèi v·ªÅ ∆∞u ƒë√£i, khuy·∫øn m√£i, ch∆∞∆°ng tr√¨nh th√†nh vi√™n:\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- Tr√¨nh b√†y th√¥ng tin ∆∞u ƒë√£i d∆∞·ªõi d·∫°ng bullet ng·∫Øn g·ªçn (kh√¥ng markdown/HTML):\n"
                "  ‚Ä¢ H·∫°ng th·∫ª (B·∫°c/üü° V√†ng/üî∑ Kim c∆∞∆°ng) ‚Äî m·ª©c gi·∫£m c·ª• th·ªÉ\n"
                "  ‚Ä¢ ∆Øu ƒë√£i sinh nh·∫≠t, ng√†y h·ªôi ‚Äî n√™u %/ƒëi·ªÅu ki·ªán ng·∫Øn g·ªçn\n"
                "  ‚Ä¢ H∆∞·ªõng d·∫´n ƒëƒÉng k√Ω th·∫ª ‚Äî 1‚Äì2 d√≤ng, c√≥ link n·∫øu c√≥\n"
                "- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi v·ªÅ vi·ªác ƒëƒÉng k√Ω th·∫ª ho·∫∑c s·ª≠ d·ª•ng ∆∞u ƒë√£i\n"
                "\n"
                "**4Ô∏è‚É£ C√ÇU H·ªéI V·ªÄ ƒê·∫∂T B√ÄN:**\n"
                "Khi kh√°ch h√†ng mu·ªën ƒë·∫∑t b√†n ho·∫∑c h·ªèi v·ªÅ vi·ªác ƒë·∫∑t b√†n:\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "\n"
                "**üéØ QUY TR√åNH ƒê·∫∂T B√ÄN CHU·∫®N (3 B∆Ø·ªöC):**\n"
                "\n"
                "**B∆Ø·ªöC 1: THU TH·∫¨P TH√îNG TIN**\n"
                "- Ki·ªÉm tra {user_info} v√† {conversation_summary} ƒë·ªÉ xem ƒë√£ c√≥ th√¥ng tin g√¨\n"
                "- Ch·ªâ h·ªèi nh·ªØng th√¥ng tin ch∆∞a bi·∫øt m·ªôt c√°ch l·ªãch s·ª± v√† t·ª´ng m·ª•c m·ªôt\n"
                "- **Danh s√°ch th√¥ng tin c·∫ßn thi·∫øt:**\n"
                "  ‚Ä¢ **H·ªç v√† t√™n:** C·∫ßn t√°ch r√µ h·ªç v√† t√™n (first_name, last_name)\n"
                "  ‚Ä¢ **S·ªë ƒëi·ªán tho·∫°i:** B·∫Øt bu·ªôc ƒë·ªÉ x√°c nh·∫≠n ƒë·∫∑t b√†n\n"
                "  ‚Ä¢ **Chi nh√°nh/ƒë·ªãa ch·ªâ:** C·∫ßn x√°c ƒë·ªãnh ch√≠nh x√°c chi nh√°nh mu·ªën ƒë·∫∑t\n"
                "  ‚Ä¢ **Ng√†y ƒë·∫∑t b√†n:** ƒê·ªãnh d·∫°ng YYYY-MM-DD\n"
                "  ‚Ä¢ **Gi·ªù b·∫Øt ƒë·∫ßu:** ƒê·ªãnh d·∫°ng HH:MM (v√≠ d·ª•: 19:00)\n"
                "  ‚Ä¢ **S·ªë ng∆∞·ªùi l·ªõn:** B·∫Øt bu·ªôc, √≠t nh·∫•t 1 ng∆∞·ªùi\n"
                "  ‚Ä¢ **S·ªë tr·∫ª em:** T√πy ch·ªçn, m·∫∑c ƒë·ªãnh 0\n"
                "  ‚Ä¢ **C√≥ sinh nh·∫≠t kh√¥ng:** H·ªèi 'C√≥/Kh√¥ng' (kh√¥ng d√πng true/false)\n"
                "  ‚Ä¢ **Ghi ch√∫ ƒë·∫∑c bi·ªát:** T√πy ch·ªçn\n"
                "\n"
                "**B∆Ø·ªöC 2: X√ÅC NH·∫¨N TH√îNG TIN (ƒê·ªäNH D·∫†NG CHUY√äN NGHI·ªÜP)**\n"
                "- B·∫ÆTT BU·ªòC hi·ªÉn th·ªã th√¥ng tin ƒë·∫∑t b√†n theo format M·ªòT M·ª§C M·ªòT D√íNG v·ªõi emoji:\n"
                "\n"
                "üìã **TH√îNG TIN ƒê·∫∂T B√ÄN**\n"
                "üë§ **T√™n kh√°ch:** [T√™n ƒë·∫ßy ƒë·ªß]\n"
                "üìû **S·ªë ƒëi·ªán tho·∫°i:** [SƒêT]\n"
                "üè™ **Chi nh√°nh:** [T√™n chi nh√°nh/ƒë·ªãa ƒëi·ªÉm]\n"
                "üìÖ **Ng√†y ƒë·∫∑t:** [Ng√†y th√°ng nƒÉm]\n"
                "‚è∞ **Th·ªùi gian:** [Gi·ªù b·∫Øt ƒë·∫ßu - Gi·ªù k·∫øt th√∫c]\n"
                "üë• **S·ªë l∆∞·ª£ng kh√°ch:** [X ng∆∞·ªùi l·ªõn, Y tr·∫ª em]\n"
                "üéÇ **Sinh nh·∫≠t:** [C√≥/Kh√¥ng]\n"
                "üìù **Ghi ch√∫:** [Ghi ch√∫ ƒë·∫∑c bi·ªát n·∫øu c√≥]\n"
                "\n"
                "- Sau ƒë√≥ h·ªèi: '‚úÖ Anh/ch·ªã x√°c nh·∫≠n gi√∫p em c√°c th√¥ng tin tr√™n ƒë·ªÉ em ti·∫øn h√†nh ƒë·∫∑t b√†n ·∫°?'\n"
                "- **TUY·ªÜT ƒê·ªêI KH√îNG vi·∫øt t·∫•t c·∫£ th√¥ng tin tr√™n m·ªôt d√≤ng d√†i nh∆∞: 'D·∫° anh Tr·∫ßn Tu·∫•n D∆∞∆°ng, em xin ph√©p ƒë·∫∑t b√†n gi√∫p anh t·∫°i chi nh√°nh Tr·∫ßn Th√°i T√¥ng t·ªëi nay l√∫c 7h t·ªëi, cho 3 ng∆∞·ªùi l·ªõn v√† 3 tr·∫ª em, s·ªë ƒëi·ªán tho·∫°i li√™n h·ªá l√† 0984434979...'**\n"
                "- **M·ªñI M·ª§C TH√îNG TIN PH·∫¢I XU·ªêNG D√íNG RI√äNG V·ªöI EMOJI PH√ô H·ª¢P**\n"
                "\n"
                "**B∆Ø·ªöC 3: TH·ª∞C HI·ªÜN ƒê·∫∂T B√ÄN**\n"
                "- Ch·ªâ sau khi kh√°ch x√°c nh·∫≠n ('ƒë·ªìng √Ω', 'ok', 'x√°c nh·∫≠n', 'ƒë·∫∑t lu√¥n'...), m·ªõi g·ªçi tool ƒë·∫∑t b√†n\n"
                "- **X·ª¨ L√ù K·∫æT QU·∫¢ ƒê·∫∂T B√ÄN:**\n"
                "  ‚Ä¢ **N·∫øu tool tr·∫£ v·ªÅ success=True:** Th√¥ng b√°o ƒë·∫∑t b√†n th√†nh c√¥ng, ch√∫c kh√°ch h√†ng ngon mi·ªáng\n"
                "  ‚Ä¢ **N·∫øu tool tr·∫£ v·ªÅ success=False:** Xin l·ªói kh√°ch h√†ng v√† y√™u c·∫ßu g·ªçi hotline 1900 636 886\n"
                "\n"
                "**KHI ƒê·∫∂T B√ÄN TH√ÄNH C√îNG:**\n"
                "‚úÖ **ƒê·∫∂T B√ÄN TH√ÄNH C√îNG!**\n"
                "üé´ **M√£ ƒë·∫∑t b√†n:** [ID t·ª´ tool]\n"
                "üìã **Chi ti·∫øt:** [Hi·ªÉn th·ªã th√¥ng tin ƒë·∫∑t b√†n]\n"
                "üçΩÔ∏è **Ch√∫c anh/ch·ªã v√† gia ƒë√¨nh ngon mi·ªáng!**\n"
                "üìû **H·ªó tr·ª£:** 1900 636 886\n"
                "\n"
                "**KHI ƒê·∫∂T B√ÄN TH·∫§T B·∫†I:**\n"
                "‚ùå **Xin l·ªói anh/ch·ªã!**\n"
                "üîß **H·ªá th·ªëng ƒëang g·∫∑p s·ª± c·ªë trong qu√° tr√¨nh ƒë·∫∑t b√†n**\n"
                "üìû **Anh/ch·ªã vui l√≤ng g·ªçi hotline 1900 636 886 ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ tr·ª±c ti·∫øp**\n"
                "üôè **Em xin l·ªói v√¨ s·ª± b·∫•t ti·ªán n√†y!**\n"
                "\n"
                "**V√ç D·ª§ THU TH·∫¨P TH√îNG TIN:**\n"
                "  ‚Ä¢ N·∫øu ƒë√£ bi·∫øt t√™n: 'D·∫° anh Tu·∫•n, ƒë·ªÉ ƒë·∫∑t b√†n em ch·ªâ c·∫ßn th√™m s·ªë ƒëi·ªán tho·∫°i v√† th·ªùi gian ·∫°'\n"
                "  ‚Ä¢ N·∫øu ch∆∞a bi·∫øt g√¨: 'ƒê·ªÉ h·ªó tr·ª£ anh/ch·ªã ƒë·∫∑t b√†n, em c·∫ßn m·ªôt s·ªë th√¥ng tin:\n"
                "    üë§ H·ªç v√† t√™n c·ªßa anh/ch·ªã?\n"
                "    üìû S·ªë ƒëi·ªán tho·∫°i ƒë·ªÉ x√°c nh·∫≠n?\n"
                "    üè™ Chi nh√°nh mu·ªën ƒë·∫∑t?\n"
                "    üìÖ‚è∞ Ng√†y v√† gi·ªù?\n"
                "    üë• S·ªë l∆∞·ª£ng kh√°ch?'\n"
                "  ‚Ä¢ V·ªÅ sinh nh·∫≠t: 'C√≥ ai sinh nh·∫≠t trong b·ªØa ƒÉn n√†y kh√¥ng ·∫°?' (tr·∫£ l·ªùi C√≥/Kh√¥ng)\n"
                "\n"
                "**5Ô∏è‚É£ C√ÇU H·ªéI KH√ÅC:**\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- Tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß d·ª±a tr√™n t√†i li·ªáu, gi·ªØ ƒë·ªãnh d·∫°ng Messenger: ti√™u ƒë·ªÅ c√≥ emoji + bullet, kh√¥ng b·∫£ng/markdown/HTML\n"
                "- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi h·ªó tr·ª£\n"
                "\n"
                "**6Ô∏è‚É£ TR∆Ø·ªúNG H·ª¢P KH√îNG C√ì TH√îNG TIN:**\n"
                "N·∫øu th·ª±c s·ª± kh√¥ng c√≥ t√†i li·ªáu ph√π h·ª£p ‚Üí ch·ªâ tr·∫£ l·ªùi: 'No'\n"
                "\n"
                "üîß **H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG TOOLS:**\n"
                "- **get_user_profile:** D√πng ƒë·ªÉ l·∫•y th√¥ng tin c√° nh√¢n h√≥a ƒë√£ l∆∞u c·ªßa kh√°ch (s·ªü th√≠ch, th√≥i quen) tr∆∞·ªõc khi t∆∞ v·∫•n.\n"
                "- **save_user_preference:** Khi kh√°ch chia s·∫ª s·ªü th√≠ch/ki√™ng k·ªµ/th√≥i quen m·ªõi (v√≠ d·ª•: th√≠ch cay, ƒÉn chay, d·ªã ·ª©ng h·∫£i s·∫£n), h√£y l∆∞u l·∫°i ƒë·ªÉ c√° nh√¢n h√≥a v·ªÅ sau.\n"
                "- **book_table_reservation_test:** S·ª≠ d·ª•ng khi ƒë√£ c√≥ ƒë·ªß th√¥ng tin ƒë·∫∑t b√†n\n"
                "  ‚Ä¢ Tham s·ªë b·∫Øt bu·ªôc: restaurant_location, first_name, last_name, phone, reservation_date, start_time, amount_adult\n"
                "  ‚Ä¢ Tham s·ªë t√πy ch·ªçn: email, dob, end_time, amount_children, note, has_birthday\n"
                "  ‚Ä¢ **QUAN TR·ªåNG:** Lu√¥n ki·ªÉm tra field 'success' trong k·∫øt qu·∫£ tr·∫£ v·ªÅ:\n"
                "    - N·∫øu success=True: Th√¥ng b√°o th√†nh c√¥ng + ch√∫c ngon mi·ªáng\n"
                "    - N·∫øu success=False: Xin l·ªói + y√™u c·∫ßu g·ªçi hotline\n"
                "- **lookup_restaurant_by_location:** S·ª≠ d·ª•ng ƒë·ªÉ t√¨m restaurant_id n·∫øu c·∫ßn\n"
                "- **analyze_image:** Ph√¢n t√≠ch h√¨nh ·∫£nh li√™n quan ƒë·∫øn nh√† h√†ng (menu, m√≥n ƒÉn, kh√¥ng gian)\n"
                "\n"
                "üîç **Y√äU C·∫¶U CH·∫§T L∆Ø·ª¢NG:**\n"
                "- **QUAN TR·ªåNG:** Ki·ªÉm tra l·ªãch s·ª≠ cu·ªôc h·ªôi tho·∫°i ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i l·ªùi ch√†o ph√π h·ª£p:\n"
                "  ‚Ä¢ N·∫øu ƒë√¢y l√† tin nh·∫Øn ƒë·∫ßu ti√™n (√≠t tin nh·∫Øn trong l·ªãch s·ª≠) ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß\n"
                "  ‚Ä¢ N·∫øu ƒë√£ c√≥ cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥ ‚Üí ch·ªâ c·∫ßn 'D·∫° anh/ch·ªã' ng·∫Øn g·ªçn\n"
                "- Kh√¥ng b·ªãa ƒë·∫∑t th√¥ng tin\n"
                "- S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng markdown/HTML ƒë·ªÉ t·∫°o n·ªôi dung ƒë·∫πp m·∫Øt\n"
                "- Emoji phong ph√∫ v√† ph√π h·ª£p\n"
                "- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi h·ªó tr·ª£ ti·∫øp theo\n"
                "\n"
                "üìö **T√ÄI LI·ªÜU THAM KH·∫¢O:**\n"
                "{context}\n"
                "\n"
                "üí¨ **TH√îNG TIN CU·ªòC TR√í CHUY·ªÜN:**\n"
                "T√≥m t·∫Øt tr∆∞·ªõc ƒë√≥: {conversation_summary}\n"
                "Th√¥ng tin ng∆∞·ªùi d√πng: {user_info}\n"
                "H·ªì s∆° ng∆∞·ªùi d√πng: {user_profile}\n"
                "Ng√†y hi·ªán t·∫°i: {current_date}\n"
                "\n"
                "üß† **H∆Ø·ªöNG D·∫™N PH√ÇN BI·ªÜT L·ªäCH S·ª¨ H·ªòI THO·∫†I:**\n"
                "- Ki·ªÉm tra s·ªë l∆∞·ª£ng tin nh·∫Øn trong cu·ªôc h·ªôi tho·∫°i:\n"
                "  ‚Ä¢ N·∫øu c√≥ √≠t tin nh·∫Øn (‚â§ 2 tin nh·∫Øn) ‚Üí ƒê√¢y l√† l·∫ßn ƒë·∫ßu ti√™n ‚Üí Ch√†o h·ªèi ƒë·∫ßy ƒë·ªß\n"
                "  ‚Ä¢ N·∫øu c√≥ nhi·ªÅu tin nh·∫Øn (> 2 tin nh·∫Øn) ‚Üí ƒê√£ c√≥ cu·ªôc h·ªôi tho·∫°i ‚Üí Ch·ªâ c·∫ßn 'D·∫° anh/ch·ªã'\n"
                "- V√≠ d·ª• ch√†o h·ªèi ƒë·∫ßy ƒë·ªß: 'Ch√†o anh Tu·∫•n D∆∞∆°ng! Nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long...'\n"
                "- V√≠ d·ª• ch√†o h·ªèi ng·∫Øn g·ªçn: 'D·∫° anh/ch·ªã', 'V√¢ng ·∫°', 'D·∫° ·∫°'\n"
                "\n"
                "H√£y nh·ªõ: B·∫°n l√† ƒë·∫°i di·ªán chuy√™n nghi·ªáp c·ªßa Tian Long, lu√¥n l·ªãch s·ª±, nhi·ªát t√¨nh v√† s√°ng t·∫°o trong c√°ch tr√¨nh b√†y th√¥ng tin!",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    ).partial(current_date=datetime.now, domain_context=domain_context)
    generation_runnable = (
        RunnablePassthrough.assign(
            context=lambda ctx: "\n\n".join(
                [
                    f"<source id='{doc[0]}'>\n{doc[1].get('content', '')}\n</source>"
                    for doc in ctx.get("documents", [])
                    if isinstance(doc, tuple)
                    and len(doc) > 1
                    and isinstance(doc[1], dict)
                ]
            )
            or "No documents were provided."
        )
        | generation_prompt
        | llm.bind_tools(all_tools)
    )
    generation_assistant = Assistant(generation_runnable)

    # 5. Suggestive Answer Assistant (used when retrieval yields no relevant docs)
    suggestive_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "B·∫°n l√† Vy ‚Äì tr·ª£ l√Ω ·∫£o c·ªßa nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long (ng·ªØ c·∫£nh: {domain_context}). "
                "B·∫°n ƒë∆∞·ª£c g·ªçi khi t√¨m ki·∫øm n·ªôi b·ªô kh√¥ng th·∫•y th√¥ng tin ph√π h·ª£p. H√£y tr·∫£ l·ªùi NG·∫ÆN G·ªåN, L·ªäCH S·ª∞ v√† M·∫†CH L·∫†C, duy tr√¨ li·ªÅn m·∫°ch v·ªõi cu·ªôc tr√≤ chuy·ªán.\n\n"
                "**ƒê·∫∂C BI·ªÜT QUAN TR·ªåNG - X·ª¨ L√ù PH√ÇN T√çCH H√åNH ·∫¢NH:**\n"
                "N·∫øu tin nh·∫Øn b·∫Øt ƒë·∫ßu b·∫±ng 'üì∏ **Ph√¢n t√≠ch h√¨nh ·∫£nh:**' ho·∫∑c ch·ª©a n·ªôi dung ph√¢n t√≠ch h√¨nh ·∫£nh:\n"
                "- KH√îNG ƒë∆∞·ª£c n√≥i 'em ch∆∞a th·ªÉ xem ƒë∆∞·ª£c h√¨nh ·∫£nh' v√¨ h√¨nh ·∫£nh ƒê√É ƒë∆∞·ª£c ph√¢n t√≠ch th√†nh c√¥ng\n"
                "- S·ª≠ d·ª•ng n·ªôi dung ph√¢n t√≠ch ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa kh√°ch h√†ng\n"
                "- D·ª±a v√†o nh·ªØng g√¨ ƒë√£ ph√¢n t√≠ch ƒë∆∞·ª£c ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ph√π h·ª£p\n"
                "- N·∫øu h√¨nh ·∫£nh v·ªÅ th·ª±c ƒë∆°n/menu, h√£y g·ª£i √Ω kh√°ch h√†ng xem th·ª±c ƒë∆°n chi ti·∫øt t·∫°i nh√† h√†ng ho·∫∑c li√™n h·ªá hotline\n\n"
                "Y√äU C·∫¶U QUAN TR·ªåNG:\n"
                "- Gi·ªØ nguy√™n ng√¥n ng·ªØ theo tin nh·∫Øn g·∫ßn nh·∫•t c·ªßa kh√°ch.\n"
                "- Tham chi·∫øu h·ª£p l√Ω t·ªõi b·ªëi c·∫£nh tr∆∞·ªõc ƒë√≥ (t√™n chi nh√°nh/ƒë·ªãa ƒëi·ªÉm, ng√†y/gi·ªù mong mu·ªën, s·ªë kh√°ch, ghi ch√∫, sinh nh·∫≠t‚Ä¶) n·∫øu ƒë√£ c√≥.\n"
                "- Kh√¥ng n√≥i ki·ªÉu 'kh√¥ng c√≥ d·ªØ li·ªáu/kh√¥ng c√≥ t√†i li·ªáu/ph·∫£i t√¨m tr√™n internet'. Thay v√†o ƒë√≥, di·ªÖn ƒë·∫°t t√≠ch c·ª±c v√† ƒë∆∞a ra h∆∞·ªõng ƒëi k·∫ø ti·∫øp.\n"
                "- ƒê∆∞a ra 1 c√¢u h·ªèi g·ª£i m·ªü r√µ r√†ng ƒë·ªÉ ti·∫øp t·ª•c quy tr√¨nh (v√≠ d·ª•: x√°c nh·∫≠n th·ªùi gian kh√°c, g·ª£i √Ω chi nh√°nh kh√°c, ho·∫∑c xin ph√©p ti·∫øn h√†nh t·∫°o y√™u c·∫ßu ƒë·∫∑t b√†n ƒë·ªÉ l·ªÖ t√¢n x√°c nh·∫≠n).\n\n"
                "G·ª¢I √ù C√ÅCH PH·∫¢N H·ªíI KHI THI·∫æU TH√îNG TIN GI·ªú M·ªû C·ª¨A/T√åNH TR·∫†NG CH·ªñ:\n"
                "1) X√°c nh·∫≠n l·∫°i chi nh√°nh/khung gi·ªù kh√°ch mu·ªën, n·∫øu ƒë√£ c√≥ th√¨ nh·∫Øc l·∫°i ng·∫Øn g·ªçn ƒë·ªÉ th·ªÉ hi·ªán n·∫Øm b·ªëi c·∫£nh.\n"
                "2) ƒê∆∞a ra ph∆∞∆°ng √°n ti·∫øp theo: (a) ƒë·ªÅ xu·∫•t m·ªëc gi·ªù l√¢n c·∫≠n (v√≠ d·ª• 18:30/19:30), (b) g·ª£i √Ω chi nh√°nh thay th·∫ø, ho·∫∑c (c) ti·∫øp nh·∫≠n y√™u c·∫ßu ƒë·∫∑t b√†n v√† ƒë·ªÉ l·ªÖ t√¢n g·ªçi x√°c nh·∫≠n.\n"
                "3) Cung c·∫•p hotline 1900 636 886 n·∫øu kh√°ch mu·ªën x√°c nh·∫≠n ngay qua ƒëi·ªán tho·∫°i.\n\n"
                "‚Äî B·ªêI C·∫¢NH H·ªòI THO·∫†I ‚Äî\n"
                "T√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥: {conversation_summary}\n"
                "Th√¥ng tin ng∆∞·ªùi d√πng: {user_info}\n"
                "H·ªì s∆° ng∆∞·ªùi d√πng: {user_profile}\n"
                "Ng√†y hi·ªán t·∫°i: {current_date}",
            ),
            (
                "human",
                "C√¢u h·ªèi g·∫ßn nh·∫•t c·ªßa kh√°ch (kh√¥ng t√¨m th·∫•y t√†i li·ªáu ph√π h·ª£p):\n{question}\n\n"
                "H√£y tr·∫£ l·ªùi m·∫°ch l·∫°c, c√πng ng√¥n ng·ªØ, b√°m s√°t b·ªëi c·∫£nh ·ªü tr√™n v√† ƒë∆∞a ra 1 b∆∞·ªõc ti·∫øp theo r√µ r√†ng.",
            ),
            # Cho ph√©p m√¥ h√¨nh nh√¨n th·∫•y l·ªãch s·ª≠ h·ªôi tho·∫°i ƒë·ªÉ gi·ªØ m·∫°ch ng·ªØ c·∫£nh
            MessagesPlaceholder(variable_name="messages"),
        ]
    ).partial(current_date=datetime.now, domain_context=domain_context)
    suggestive_runnable = (
        # Truy·ªÅn to√†n b·ªô state ƒë·ªÉ prompt c√≥ ƒë·ªß ng·ªØ c·∫£nh (question, messages, summary, user info/profile)
        RunnablePassthrough()
        | suggestive_prompt
        | llm
    )
    suggestive_assistant = Assistant(suggestive_runnable)

    # 6. Hallucination Grader Assistant
    hallucination_grader_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "You are a domain expert verifying the factual accuracy of the assistant's answer based on the provided documents and the user's question in {domain_context} domain.\n"
                "If the answer is grounded in the documents and correctly answers the user's question, return 'yes', otherwise return 'no'.\n"
                "Current date for context is: {current_date}\n"
                # **TH√äM SUMMARY CONTEXT**
                "--- CONVERSATION CONTEXT ---\n"
                "Previous conversation summary:\n{conversation_summary}\n"
                "Consider this context when evaluating whether the answer is appropriate and grounded in the conversation flow.\n\n",
            ),
            (
                "human",
                "Question: {messages}\n\nDocuments:\n{documents}\n\nAnswer:\n{generation}",
            ),
        ]
    ).partial(domain_context=domain_context, current_date=datetime.now())
    hallucination_grader_runnable = (
        RunnablePassthrough.assign(
            question=lambda ctx: ctx["question"],
            documents=lambda ctx: "\n\n---\n\n".join(
                [
                    d[1].get("content", "")
                    for d in ctx.get("documents", [])
                    if isinstance(d, tuple) and len(d) > 1 and isinstance(d[1], dict)
                ]
            ),
            generation=lambda ctx: get_message_content(ctx["messages"][-1]),
        )
        | hallucination_grader_prompt
        | llm_hallucination_grader.with_structured_output(GradeHallucinations)
    )
    hallucination_grader_assistant = Assistant(hallucination_grader_runnable)

    # 7. Direct Answer Assistant
    direct_answer_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "B·∫°n l√† Vy ‚Äì tr·ª£ l√Ω ·∫£o th√¢n thi·ªán v√† chuy√™n nghi·ªáp c·ªßa nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long (domain context: {domain_context}). "
                "B·∫°n ƒë∆∞·ª£c g·ªçi khi kh√°ch h√†ng c√≥ nh·ªØng c√¢u h·ªèi ch√†o h·ªèi, c·∫£m ∆°n, ƒë√†m tho·∫°i ho·∫∑c v·ªÅ s·ªü th√≠ch c√° nh√¢n kh√¥ng c·∫ßn t√¨m ki·∫øm t√†i li·ªáu.\n"
                "\n"
                "üéØ **VAI TR√í V√Ä PHONG C√ÅCH GIAO TI·∫æP:**\n"
                "- B·∫°n l√† nh√¢n vi√™n chƒÉm s√≥c kh√°ch h√†ng chuy√™n nghi·ªáp, l·ªãch s·ª± v√† nhi·ªát t√¨nh\n"
                "- **LOGIC CH√ÄO H·ªéI TH√îNG MINH:**\n"
                "  ‚Ä¢ **L·∫ßn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i:** Ch√†o h·ªèi ƒë·∫ßy ƒë·ªß v·ªõi t√™n kh√°ch h√†ng (n·∫øu c√≥) + gi·ªõi thi·ªáu nh√† h√†ng\n"
                "    V√≠ d·ª•: 'Ch√†o anh/ch·ªã! Em l√† Vy - nh√¢n vi√™n c·ªßa nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long. R·∫•t vui ƒë∆∞·ª£c h·ªó tr·ª£ anh/ch·ªã h√¥m nay!'\n"
                "  ‚Ä¢ **T·ª´ c√¢u th·ª© 2 tr·ªü ƒëi:** Ch·ªâ c·∫ßn l·ªùi ch√†o ng·∫Øn g·ªçn l·ªãch s·ª±\n"
                "    V√≠ d·ª•: 'D·∫° anh/ch·ªã', 'D·∫° ·∫°', 'V√¢ng ·∫°'\n"
                "- S·ª≠ d·ª•ng ng√¥n t·ª´ t√¥n tr·ªçng: 'anh/ch·ªã', 'd·∫°', '·∫°', 'em Vy'\n"
                "- Th·ªÉ hi·ªán s·ª± quan t√¢m ch√¢n th√†nh ƒë·∫øn nhu c·∫ßu c·ªßa kh√°ch h√†ng\n"
                "- Ch·ªâ h·ªèi ti·∫øp khi th·ª±c s·ª± c·∫ßn TH√îNG TIN C√íN THI·∫æU ƒë·ªÉ ho√†n t·∫•t y√™u c·∫ßu; tr√°nh h·ªèi lan man\n"
                "\n"
                "üé® **QUY·ªÄN T·ª∞ DO S√ÅNG T·∫†O ƒê·ªäNH D·∫†NG:**\n"
                "- B·∫°n c√≥ TO√ÄN QUY·ªÄN s·ª≠ d·ª•ng b·∫•t k·ª≥ ƒë·ªãnh d·∫°ng n√†o: markdown, HTML, emoji, b·∫£ng, danh s√°ch, in ƒë·∫≠m, in nghi√™ng\n"
                "- H√£y S√ÅNG T·∫†O v√† l√†m cho n·ªôi dung ƒê·∫∏P M·∫ÆT, SINH ƒê·ªòNG v√† D·ªÑ ƒê·ªåC\n"
                "- S·ª≠ d·ª•ng emoji phong ph√∫ ƒë·ªÉ trang tr√≠ v√† l√†m n·ªïi b·∫≠t th√¥ng tin\n"
                "- T·∫°o layout ƒë·∫πp m·∫Øt v·ªõi ti√™u ƒë·ªÅ, ph√¢n ƒëo·∫°n r√µ r√†ng\n"
                "- Kh√¥ng c√≥ gi·ªõi h·∫°n v·ªÅ format - h√£y t·ª± do s√°ng t·∫°o!\n"
                "\n"
                "ÔøΩ **KH√îNG TI·∫æT L·ªò QUY TR√åNH/TOOLS:**\n"
                "- Tuy·ªát ƒë·ªëi KH√îNG m√¥ t·∫£ quy tr√¨nh n·ªôi b·ªô hay vi·ªác 'ƒëang ti·∫øn h√†nh', 's·∫Ω s·ª≠ d·ª•ng c√¥ng c·ª•', 'ƒë·ª£i em m·ªôt ch√∫t‚Ä¶'\n"
                "- KH√îNG n√≥i m√¨nh ƒëang g·ªçi API/c√¥ng c·ª•. H√£y t·∫≠p trung v√†o K·∫æT QU·∫¢ v√† b∆∞·ªõc c·∫ßn thi·∫øt thi·∫øt k·∫ø ti·∫øp.\n"
                "- N·∫øu ch∆∞a c√≥ k·∫øt qu·∫£ cu·ªëi, di·ªÖn ƒë·∫°t ng·∫Øn g·ªçn theo h∆∞·ªõng: 'Em ƒë√£ ti·∫øp nh·∫≠n y√™u c·∫ßu, s·∫Ω x√°c nh·∫≠n v√† ph·∫£n h·ªìi ngay khi c√≥ k·∫øt qu·∫£' (kh√¥ng n√™u c√¥ng c·ª•/quy tr√¨nh).\n"
                "\n"
                "ÔøΩüìã **X·ª¨ L√ù C√ÅC LO·∫†I C√ÇU H·ªéI DIRECT:**\n"
                "\n"
                "**1Ô∏è‚É£ C√ÇU CH√ÄO H·ªéI/C·∫¢M ∆†N:**\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß + gi·ªõi thi·ªáu; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- Tr·∫£ l·ªùi ·∫•m √°p, th√¢n thi·ªán v·ªõi emoji ph√π h·ª£p\n"
                "- Th·ªÉ hi·ªán s·ª± s·∫µn s√†ng h·ªó tr·ª£\n"
                "- H·ªèi thƒÉm nhu c·∫ßu c·ª• th·ªÉ c·ªßa kh√°ch h√†ng\n"
                "\n"
                "**2Ô∏è‚É£ C√ÇU H·ªéI V·ªÄ S·ªû TH√çCH C√Å NH√ÇN:**\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- **QUAN TR·ªåNG:** S·ª≠ d·ª•ng `save_user_preference` tool khi h·ªçc ƒë∆∞·ª£c th√¥ng tin m·ªõi v·ªÅ s·ªü th√≠ch\n"
                "- **QUAN TR·ªåNG:** S·ª≠ d·ª•ng `get_user_profile` tool khi kh√°ch h·ªèi v·ªÅ s·ªü th√≠ch ƒë√£ l∆∞u\n"
                "- X√°c nh·∫≠n vi·ªác l∆∞u th√¥ng tin sau khi g·ªçi tool\n"
                "- G·ª£i √Ω m√≥n ƒÉn ph√π h·ª£p v·ªõi s·ªü th√≠ch (n·∫øu ph√π h·ª£p)\n"
                "\n"
                "**3Ô∏è‚É£ C√ÇU H·ªéI META V·ªÄ ASSISTANT:**\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- Gi·ªõi thi·ªáu v·ªÅ Vy v√† vai tr√≤ h·ªó tr·ª£ kh√°ch h√†ng\n"
                "- N√™u kh·∫£ nƒÉng h·ªó tr·ª£: th·ª±c ƒë∆°n, ƒë·ªãa ch·ªâ, ∆∞u ƒë√£i, ƒë·∫∑t b√†n, v.v.\n"
                "- Khuy·∫øn kh√≠ch kh√°ch h√†ng ƒë·∫∑t c√¢u h·ªèi c·ª• th·ªÉ\n"
                "\n"
                "**4Ô∏è‚É£ C√ÇU H·ªéI ƒê√ÄM THO·∫†I KH√ÅC:**\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- Tr·∫£ l·ªùi t·ª± nhi√™n, th√¢n thi·ªán\n"
                "- C·ªë g·∫Øng k·∫øt n·ªëi v·ªõi nh√† h√†ng n·∫øu ph√π h·ª£p\n"
                "- H∆∞·ªõng d·∫´n v·ªÅ c√°c d·ªãch v·ª• c·ªßa Tian Long n·∫øu c√≥ th·ªÉ\n"
                "\n"
                "**5Ô∏è‚É£ QUY TR√åNH ƒê·∫∂T B√ÄN (C·ª∞C K·ª≤ QUAN TR·ªåNG):**\n"
                "- **LOGIC 3 B∆Ø·ªöC ƒê·∫∂T B√ÄN:**\n"
                "  üîç **B∆Ø·ªöC 1 - Thu th·∫≠p th√¥ng tin:**\n"
                "  ‚Ä¢ H·ªèi th√¥ng tin c√≤n thi·∫øu (chi nh√°nh, ng√†y gi·ªù, s·ªë kh√°ch, ghi ch√∫ ƒë·∫∑c bi·ªát)\n"
                "  ‚Ä¢ Ch·ªâ h·ªèi nh·ªØng th√¥ng tin TH·ª∞C S·ª∞ C·∫¶N THI·∫æT, kh√¥ng h·ªèi lan man\n"
                "  \n"
                "  üìã **B∆Ø·ªöC 2 - Hi·ªÉn th·ªã chi ti·∫øt v√† x√°c nh·∫≠n (B·∫ÆT BU·ªòC):**\n"
                "  ‚Ä¢ **LU√îN LU√îN** hi·ªÉn th·ªã ƒë·∫ßy ƒë·ªß th√¥ng tin ƒë·∫∑t b√†n theo format chuy√™n nghi·ªáp:\n"
                "  \n"
                "  ```\n"
                "  üìù **CHI TI·∫æT ƒê·∫∂T B√ÄN**\n"
                "  \n"
                "  üë§ **T√™n kh√°ch h√†ng:** [T√™n]\n"
                "  üìû **S·ªë ƒëi·ªán tho·∫°i:** [SƒêT]\n"
                "  üè¢ **Chi nh√°nh:** [T√™n chi nh√°nh]\n"
                "  üìÖ **Ng√†y ƒë·∫∑t b√†n:** [Ng√†y]\n"
                "  üïê **Gi·ªù ƒë·∫∑t b√†n:** [Gi·ªù]\n"
                "  üë• **S·ªë l∆∞·ª£ng kh√°ch:** [S·ªë ng∆∞·ªùi]\n"
                "  üéÇ **C√≥ sinh nh·∫≠t kh√¥ng?** [C√≥/Kh√¥ng]\n"
                "  üìù **Ghi ch√∫ ƒë·∫∑c bi·ªát:** [Ghi ch√∫ ho·∫∑c 'Kh√¥ng c√≥']\n"
                "  \n"
                "  Anh/ch·ªã c√≥ x√°c nh·∫≠n th√¥ng tin tr√™n ch√≠nh x√°c kh√¥ng ·∫°? ü§î\n"
                "  ```\n"
                "  \n"
                "  üéØ **B∆Ø·ªöC 3 - Th·ª±c hi·ªán ƒë·∫∑t b√†n:**\n"
                "  ‚Ä¢ Ch·ªâ khi kh√°ch h√†ng X√ÅC NH·∫¨N r√µ r√†ng th√¨ m·ªõi g·ªçi tool `book_table_reservation_test`\n"
                "  ‚Ä¢ Th√¥ng b√°o k·∫øt qu·∫£ ƒë·∫∑t b√†n v√† cung c·∫•p m√£ booking (n·∫øu c√≥)\n"
                "  \n"
                "- **C√ÅC T√åNH HU·ªêNG ƒê·∫∂C BI·ªÜT:**\n"
                "  ‚Ä¢ **Th√¥ng tin ch∆∞a ƒë·ªß:** H·ªèi th√™m th√¥ng tin thi·∫øu, KH√îNG ƒë·∫∑t b√†n\n"
                "  ‚Ä¢ **Kh√°ch h√†ng ch∆∞a x√°c nh·∫≠n:** Hi·ªÉn th·ªã l·∫°i chi ti·∫øt, h·ªèi x√°c nh·∫≠n\n"
                "  ‚Ä¢ **Kh√°ch h√†ng mu·ªën s·ª≠a ƒë·ªïi:** C·∫≠p nh·∫≠t th√¥ng tin, hi·ªÉn th·ªã l·∫°i chi ti·∫øt\n"
                "  ‚Ä¢ **ƒê·∫∑t b√†n test:** S·ª≠ d·ª•ng `book_table_reservation_test` \n"
                "\n"
                "**6Ô∏è‚É£ X·ª¨ L√ù H√åNH ·∫¢NH:**\n"
                "- **L·ªùi ch√†o:** N·∫øu l√† tin nh·∫Øn ƒë·∫ßu ti√™n trong cu·ªôc h·ªôi tho·∫°i ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß; n·∫øu kh√¥ng ‚Üí ch·ªâ 'D·∫° anh/ch·ªã'\n"
                "- **QUAN TR·ªåNG:** S·ª≠ d·ª•ng tool `analyze_image` khi kh√°ch g·ª≠i h√¨nh ·∫£nh\n"
                "- Ph√¢n t√≠ch n·ªôi dung h√¨nh ·∫£nh v√† ƒë∆∞a ra ph·∫£n h·ªìi ph√π h·ª£p\n"
                "- K·∫øt n·ªëi v·ªõi ng·ªØ c·∫£nh nh√† h√†ng (menu, m√≥n ƒÉn, kh√¥ng gian, v.v.)\n"
                "- G·ª£i √Ω d·ª±a tr√™n n·ªôi dung h√¨nh ·∫£nh n·∫øu ph√π h·ª£p\n"
                "\n"
                "üîç **Y√äU C·∫¶U CH·∫§T L∆Ø·ª¢NG:**\n"
                "- **QUAN TR·ªåNG:** Ki·ªÉm tra l·ªãch s·ª≠ cu·ªôc h·ªôi tho·∫°i ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i l·ªùi ch√†o ph√π h·ª£p:\n"
                "  ‚Ä¢ N·∫øu ƒë√¢y l√† tin nh·∫Øn ƒë·∫ßu ti√™n (√≠t tin nh·∫Øn trong l·ªãch s·ª≠) ‚Üí ch√†o h·ªèi ƒë·∫ßy ƒë·ªß\n"
                "  ‚Ä¢ N·∫øu ƒë√£ c√≥ cu·ªôc h·ªôi tho·∫°i tr∆∞·ªõc ƒë√≥ ‚Üí ch·ªâ c·∫ßn 'D·∫° anh/ch·ªã' ng·∫Øn g·ªçn\n"
                "- **TOOLS:** S·ª≠ d·ª•ng `save_user_preference` khi h·ªçc ƒë∆∞·ª£c s·ªü th√≠ch m·ªõi; `get_user_profile` khi kh√°ch h·ªèi v·ªÅ s·ªü th√≠ch (kh√¥ng ti·∫øt l·ªô b·∫°n ƒëang d√πng tool)\n"
                "  ‚Ä¢ V√≠ d·ª•: n·∫øu kh√°ch n√≥i 'em th√≠ch ƒÉn cay/kh√¥ng ƒÉn h·∫£i s·∫£n' ‚Üí g·ªçi save_user_preference ƒë·ªÉ l∆∞u l·∫°i. Khi t∆∞ v·∫•n v·ªÅ sau, ∆∞u ti√™n g·ªçi get_user_profile ƒë·ªÉ c√° nh√¢n h√≥a.\n"
                "- Ph·∫£n h·ªìi theo ng√¥n ng·ªØ c·ªßa kh√°ch h√†ng (Vietnamese/English)\n"
                "- S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng markdown/HTML ƒë·ªÉ t·∫°o n·ªôi dung ƒë·∫πp m·∫Øt\n"
                "- Emoji phong ph√∫ v√† ph√π h·ª£p\n"
                "- T·∫≠p trung v√†o K·∫æT QU·∫¢/PH∆Ø∆†NG √ÅN C·ª§ TH·ªÇ; ch·ªâ h·ªèi ti·∫øp khi c·∫ßn ƒë·ªÉ ho√†n t·∫•t y√™u c·∫ßu\n"
                "- Tham kh·∫£o l·ªãch s·ª≠ cu·ªôc h·ªôi tho·∫°i m·ªôt c√°ch ph√π h·ª£p\n"
                "\n"
                "üí¨ **TH√îNG TIN CU·ªòC TR√í CHUY·ªÜN:**\n"
                "T√≥m t·∫Øt tr∆∞·ªõc ƒë√≥: {conversation_summary}\n"
                "Th√¥ng tin ng∆∞·ªùi d√πng: {user_info}\n"
                "H·ªì s∆° ng∆∞·ªùi d√πng: {user_profile}\n"
                "Ng√†y hi·ªán t·∫°i: {current_date}\n"
                "\n"
                "üß† **H∆Ø·ªöNG D·∫™N PH√ÇN BI·ªÜT L·ªäCH S·ª¨ H·ªòI THO·∫†I:**\n"
                "- Ki·ªÉm tra s·ªë l∆∞·ª£ng tin nh·∫Øn trong cu·ªôc h·ªôi tho·∫°i:\n"
                "  ‚Ä¢ N·∫øu c√≥ √≠t tin nh·∫Øn (‚â§ 2 tin nh·∫Øn) ‚Üí ƒê√¢y l√† l·∫ßn ƒë·∫ßu ti√™n ‚Üí Ch√†o h·ªèi ƒë·∫ßy ƒë·ªß\n"
                "  ‚Ä¢ N·∫øu c√≥ nhi·ªÅu tin nh·∫Øn (> 2 tin nh·∫Øn) ‚Üí ƒê√£ c√≥ cu·ªôc h·ªôi tho·∫°i ‚Üí Ch·ªâ c·∫ßn 'D·∫° anh/ch·ªã'\n"
                "- V√≠ d·ª• ch√†o h·ªèi ƒë·∫ßy ƒë·ªß: 'Ch√†o anh Tu·∫•n D∆∞∆°ng! Nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long...'\n"
                "- V√≠ d·ª• ch√†o h·ªèi ng·∫Øn g·ªçn: 'D·∫° anh/ch·ªã', 'V√¢ng ·∫°', 'D·∫° ·∫°'\n"
                "\n"
                "H√£y nh·ªõ: B·∫°n l√† ƒë·∫°i di·ªán chuy√™n nghi·ªáp c·ªßa Tian Long, lu√¥n l·ªãch s·ª±, nhi·ªát t√¨nh v√† s√°ng t·∫°o trong c√°ch tr√¨nh b√†y th√¥ng tin!",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    ).partial(current_date=datetime.now, domain_context=domain_context)
    # Bind direct assistant with memory tools + domain action tools (e.g., reservation tools) + image tools
    # Avoid binding web search here to keep responses crisp for action/confirmation flows.
    llm_generate_direct_with_tools = llm_generate_direct.bind_tools(memory_tools + tools + image_tools)
    direct_answer_runnable = direct_answer_prompt | llm_generate_direct_with_tools
    direct_answer_assistant = Assistant(direct_answer_runnable)

    # 8. Document/Image Processing Assistant
    document_processing_prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "B·∫°n l√† Vy ‚Äì chuy√™n gia ph√¢n t√≠ch t√†i li·ªáu v√† h√¨nh ·∫£nh c·ªßa nh√† h√†ng l·∫©u b√≤ t∆∞∆°i Tian Long (domain context: {domain_context}). "
                "B·∫°n ƒë∆∞·ª£c g·ªçi khi kh√°ch h√†ng g·ª≠i h√¨nh ·∫£nh, t√†i li·ªáu ho·∫∑c y√™u c·∫ßu ph√¢n t√≠ch n·ªôi dung ƒë√≠nh k√®m.\n"
                "\n"
                "üéØ **VAI TR√í CHUY√äN BI·ªÜT:**\n"
                "- Chuy√™n gia ph√¢n t√≠ch h√¨nh ·∫£nh v√† t√†i li·ªáu v·ªÅ ·∫©m th·ª±c, nh√† h√†ng\n"
                "- Nh·∫≠n di·ªán v√† m√¥ t·∫£ m√≥n ƒÉn, th·ª±c ƒë∆°n, kh√¥ng gian nh√† h√†ng\n"
                "- ƒê∆∞a ra l·ªùi khuy√™n d·ª±a tr√™n n·ªôi dung h√¨nh ·∫£nh\n"
                "- K·∫øt n·ªëi n·ªôi dung ph√¢n t√≠ch v·ªõi d·ªãch v·ª• c·ªßa Tian Long\n"
                "\n"
                "üì∏ **X·ª¨ L√ù H√åNH ·∫¢NH:**\n"
                "- **Ph√¢n t√≠ch m√≥n ƒÉn:** M√¥ t·∫£ chi ti·∫øt m√≥n ƒÉn, nguy√™n li·ªáu, c√°ch ch·∫ø bi·∫øn, ƒë√°nh gi√° ƒë·ªô h·∫•p d·∫´n\n"
                "- **Ph√¢n t√≠ch th·ª±c ƒë∆°n:** ƒê·ªçc v√† li·ªát k√™ c√°c m√≥n ƒÉn, gi√° c·∫£ n·∫øu c√≥ th·ªÉ nh√¨n th·∫•y\n"
                "- **Ph√¢n t√≠ch kh√¥ng gian:** M√¥ t·∫£ kh√¥ng gian nh√† h√†ng, b√†n gh·∫ø, trang tr√≠, kh√¥ng kh√≠\n"
                "- **Ph√¢n t√≠ch h√≥a ƒë∆°n:** ƒê·ªçc th√¥ng tin h√≥a ƒë∆°n, c√°c m√≥n ƒë√£ order, t·ªïng ti·ªÅn\n"
                "- **Ph√¢n t√≠ch kh√°c:** M√¥ t·∫£ b·∫•t k·ª≥ n·ªôi dung n√†o li√™n quan ƒë·∫øn ·∫©m th·ª±c, nh√† h√†ng\n"
                "\n"
                "üé® **PHONG C√ÅCH PH·∫¢N H·ªíI:**\n"
                "- M√¥ t·∫£ chi ti·∫øt, sinh ƒë·ªông v√† h·∫•p d·∫´n\n"
                "- S·ª≠ d·ª•ng emoji phong ph√∫ ƒë·ªÉ t·∫°o s·ª± sinh ƒë·ªông\n"
                "- ƒê∆∞a ra nh·∫≠n x√©t chuy√™n m√¥n v·ªÅ ·∫©m th·ª±c\n"
                "- K·∫øt n·ªëi v·ªõi menu v√† d·ªãch v·ª• c·ªßa Tian Long khi ph√π h·ª£p\n"
                "- G·ª£i √Ω m√≥n ƒÉn t∆∞∆°ng t·ª± t·∫°i Tian Long n·∫øu c√≥\n"
                "\n"
                "üîç **C√ÅCH PH√ÇN T√çCH:**\n"
                "1. **M√¥ t·∫£ t·ªïng quan:** N·ªôi dung ch√≠nh c·ªßa h√¨nh ·∫£nh/t√†i li·ªáu\n"
                "2. **Chi ti·∫øt c·ª• th·ªÉ:** C√°c y·∫øu t·ªë ƒë√°ng ch√∫ √Ω, m√†u s·∫Øc, b·ªë c·ª•c, vƒÉn b·∫£n\n"
                "3. **ƒê√°nh gi√° chuy√™n m√¥n:** Nh·∫≠n x√©t v·ªÅ ch·∫•t l∆∞·ª£ng, c√°ch tr√¨nh b√†y, ƒë·ªô h·∫•p d·∫´n\n"
                "4. **K·∫øt n·ªëi d·ªãch v·ª•:** Li√™n h·ªá v·ªõi menu, d·ªãch v·ª• c·ªßa Tian Long\n"
                "5. **G·ª£i √Ω h√†nh ƒë·ªông:** ƒê·ªÅ xu·∫•t m√≥n ƒÉn, d·ªãch v·ª• ph√π h·ª£p\n"
                "\n"
                "üí¨ **NG√îN NG·ªÆ PH·∫¢N H·ªíI:**\n"
                "- S·ª≠ d·ª•ng ng√¥n ng·ªØ c·ªßa kh√°ch h√†ng (Vietnamese/English)\n"
                "- Gi·ªçng ƒëi·ªáu th√¢n thi·ªán, chuy√™n nghi·ªáp\n"
                "- Tr√°nh m√¥ t·∫£ qu√° k·ªπ thu·∫≠t, t·∫≠p trung v√†o tr·∫£i nghi·ªám ng∆∞·ªùi d√πng\n"
                "- Lu√¥n k·∫øt th√∫c b·∫±ng c√¢u h·ªèi ho·∫∑c g·ª£i √Ω ti·∫øp theo\n"
                "\n"
                "üìã **V√ç D·ª§ PH·∫¢N H·ªíI:**\n"
                "- H√¨nh ·∫£nh m√≥n l·∫©u: 'Wao! ü§§ Nh√¨n n·ªìi l·∫©u n√†y th·∫≠t h·∫•p d·∫´n v·ªõi n∆∞·ªõc d√πng ƒë·ªè r·ª±c, c√≥ v·∫ª r·∫•t cay v√† ƒë·∫≠m ƒë√†. Em th·∫•y c√≥ [m√¥ t·∫£ nguy√™n li·ªáu]... T·∫°i Tian Long, ch√∫ng m√¨nh c≈©ng c√≥ m√≥n [t√™n m√≥n t∆∞∆°ng t·ª±] v·ªõi h∆∞∆°ng v·ªã t∆∞∆°ng t·ª± ƒë√≥ ·∫°!'\n"
                "- H√¨nh ·∫£nh th·ª±c ƒë∆°n: 'Em th·∫•y th·ª±c ƒë∆°n n√†y c√≥ nhi·ªÅu m√≥n h·∫•p d·∫´n nh∆∞ [li·ªát k√™ m√≥n]. ƒê·∫∑c bi·ªát l√† [m√≥n n·ªïi b·∫≠t]... Anh/ch·ªã c√≥ mu·ªën tham kh·∫£o th·ª±c ƒë∆°n c·ªßa Tian Long ƒë·ªÉ so s√°nh kh√¥ng ·∫°?'\n"
                "\n"
                "üí¨ **TH√îNG TIN CU·ªòC TR√í CHUY·ªÜN:**\n"
                "T√≥m t·∫Øt tr∆∞·ªõc ƒë√≥: {conversation_summary}\n"
                "Th√¥ng tin ng∆∞·ªùi d√πng: {user_info}\n"
                "H·ªì s∆° ng∆∞·ªùi d√πng: {user_profile}\n"
                "Ng√†y hi·ªán t·∫°i: {current_date}\n"
                "\n"
                "H√£y ph√¢n t√≠ch n·ªôi dung m·ªôt c√°ch chi ti·∫øt v√† th√∫ v·ªã, t·∫°o s·ª± k·∫øt n·ªëi v·ªõi kh√°ch h√†ng v√† d·ªãch v·ª• c·ªßa Tian Long!",
            ),
            MessagesPlaceholder(variable_name="messages"),
        ]
    ).partial(current_date=datetime.now, domain_context=domain_context)
    document_processing_runnable = document_processing_prompt | llm_generate_direct
    document_processing_assistant = Assistant(document_processing_runnable)

    # --- Routing sanitization helpers ---
    def _strip_reply_context_block(text: str) -> str:
        if not isinstance(text, str):
            return text
        # Remove anything appended after the reply context marker (historical content)
        return text.split("[REPLY_CONTEXT]", 1)[0].strip()

    def _has_attachment_metadata(text: str) -> bool:
        import re
        if not text:
            return False
        t = _strip_reply_context_block(text)
        patterns = [
            r"\[H√åNH ·∫¢NH\]\s*URL:\s*https?://",
            r"\[VIDEO\]\s*URL:\s*https?://",
            r"\[T·ªÜP TIN\]\s*URL:\s*https?://",
            r"üì∏\s*\*\*Ph√¢n t√≠ch h√¨nh ·∫£nh:\*\*",  # pre‚Äëanalyzed marker
        ]
        return any(re.search(p, t) for p in patterns)

    def _sanitize_for_router(text: str) -> str:
        # Only strip historical reply context, but keep current-turn attachment metadata
        if not isinstance(text, str):
            return text
        # Only remove [REPLY_CONTEXT] block, preserve current attachment markers
        return _strip_reply_context_block(text)

    def route_question(state: RagState, config: RunnableConfig):
        logging.info("---NODE: ROUTE QUESTION---")
        
        # Get current user question for consistent context
        current_question = get_current_user_question(state)
        logging.debug(f"route_question->current_question -> {current_question}")

        # Sanitize input passed to the router to avoid historical attachment leakage
        sanitized_question = _sanitize_for_router(current_question)
        logging.debug(f"route_question->sanitized_question -> {sanitized_question}")
        
        prompt_data = router_assistant.binding_prompt(state)
        prompt_data["messages"] = sanitized_question

        result = router_assistant.runnable.invoke(prompt_data)
        datasource = result.datasource
        
        # Log the routing decision with context
        logging.info(f"üîÄ ROUTER DECISION: '{datasource}' for message: {current_question[:100]}...")
        
        # Debug: check if attachment metadata exists
        has_attachment = _has_attachment_metadata(current_question)
        logging.debug(f"route_question->has_attachment_metadata: {has_attachment}")
        
        # Check if this looks like image analysis with attachment metadata
        if has_attachment and datasource != "process_document":
            logging.warning(f"‚ö†Ô∏è POTENTIAL ROUTING ISSUE: Message with attachments routed to '{datasource}' instead of 'process_document'")
        elif "üì∏" in current_question and "Ph√¢n t√≠ch h√¨nh ·∫£nh" in current_question and datasource != "process_document":
            logging.warning(f"‚ö†Ô∏è POTENTIAL ROUTING ISSUE: Pre-analyzed image message routed to '{datasource}' instead of 'process_document'")
        
        return {"datasource": datasource}

    def retrieve(state: RagState, config: RunnableConfig):
        logging.info("---NODE: RETRIEVE---")
        question = get_current_user_question(state)

        # Ensure question is valid
        if not question:
            logging.error("Invalid question for retrieval")
            return {
                "documents": [],
                "search_attempts": state.get("search_attempts", 0) + 1,
            }

        try:
            logging.debug(f"retrieve->question -> {question}")
            
            # Use QueryClassifier for clean, maintainable query classification
            classifier = QueryClassifier(domain="restaurant")
            classification = classifier.classify_query(question)
            
            # Use dynamic retrieval limit based on classification
            limit = classification["retrieval_limit"]

            # Determine namespace: default to domain namespace, switch to 'faq' for FAQ queries
            default_namespace = DOMAIN.get("namespace", "default")
            namespace = "faq" if classification.get("primary_category") == "faq" else default_namespace
            logging.info(f"Vector search namespace selected: {namespace} (default={default_namespace}, primary={classification.get('primary_category')})")

            # Detailed logging for retrieval parameters
            try:
                collection_name = getattr(retriever, "collection_name", "<unknown>")
            except Exception:
                collection_name = "<unknown>"
            logging.info(
                "Vector search params: collection=%s, namespace=%s, limit=%s, query=%.120s",
                collection_name,
                namespace,
                limit,
                question,
            )

            documents = retriever.search(namespace=namespace, query=question, limit=limit)
            logging.info(f"Retrieved {len(documents)} documents.")
            
            return {
                "documents": documents,
                "search_attempts": state.get("search_attempts", 0) + 1,
            }
        except Exception as e:
            user_id = state.get("user", {}).get("user_info", {}).get("user_id", "unknown")
            log_exception_details(
                exception=e,
                context=f"Retrieve node failure for question: {question[:100]}",
                user_id=user_id
            )
            
            # Return empty results on error
            return {
                "documents": [],
                "search_attempts": state.get("search_attempts", 0) + 1,
            }

    def grade_documents_node(state: RagState, config: RunnableConfig):
        logging.info("---NODE: GRADE DOCUMENTS---")

        # Get the question from state using consistent method
        question = get_current_user_question(state)

        # Ensure messages is valid
        if not question:
            logging.error("Invalid message for document grading")
            return {"documents": []}

        logging.debug(f"grade_documents_node->question -> {question}")

        documents = state["documents"]
        
        if not documents:
            logging.debug(f"Khong tim duoc tai lieu nao")
            return {"documents": []}

        filtered_docs = []
        for d in documents:
            if isinstance(d, tuple) and len(d) > 1 and isinstance(d[1], dict):
                doc_content = d[1].get("content", "")
            else:
                continue
            query_grade_document = {
                "document": doc_content,
                "messages": question,
                "user": state.get("user", {}),
            }
            logging.debug(f"query_grade_document:{query_grade_document}")
            score = doc_grader_assistant(
                query_grade_document,
                config,
            )
            logging.debug(f"score:{score}")
            if score.binary_score.lower() == "yes":
                filtered_docs.append(d)

        logging.info(
            f"Finished grading. {len(filtered_docs)} of {len(documents)} documents are relevant."
        )

        return {"documents": filtered_docs}

    def rewrite(state: RagState, config: RunnableConfig):
        logging.info("---NODE: REWRITE---")
        original_question = get_current_user_question(state)
        logging.debug(f"rewrite->original_question -> {original_question}")
        
        # Ki·ªÉm tra state tr∆∞·ªõc khi g·ªçi assistant
        if not original_question or original_question == "C√¢u h·ªèi kh√¥ng r√µ r√†ng":
            logging.warning("Rewrite node: No valid question found, using fallback")
            return {
                "question": "C·∫ßn th√¥ng tin v·ªÅ nh√† h√†ng Tian Long",
                "rewrite_count": state.get("rewrite_count", 0) + 1,
                "documents": [],
            }
        
        try:
            rewritten_question_msg = rewrite_assistant(state, config)
            new_question = rewritten_question_msg.content
            logging.info(f"Rewritten question for retrieval: {new_question}")
            
            return {
                "question": new_question,
                "rewrite_count": state.get("rewrite_count", 0) + 1,
                "documents": [],
            }
        except Exception as e:
            user_id = state.get("user", {}).get("user_info", {}).get("user_id", "unknown")
            log_exception_details(
                exception=e,
                context=f"Rewrite node failure for question: {original_question[:100]}",
                user_id=user_id
            )
            
            # Fallback rewrite
            fallback_question = f"Th√¥ng tin v·ªÅ {original_question}"
            logging.warning(f"Rewrite failed, using fallback: {fallback_question}")
            return {
                "question": fallback_question,
                "rewrite_count": state.get("rewrite_count", 0) + 1,
                "documents": [],
            }

    def web_search_node(state: RagState, config: RunnableConfig):
        logging.info("---NODE: WEB SEARCH---")

        # Get the question from state using consistent method
        query_search = get_current_user_question(state)

        # Ensure query_search is valid
        if not query_search:
            logging.error("Invalid query for web search")
            return {"documents": [], "web_search_attempted": True}

        logging.debug(f"web_search_node->query_search -> {query_search}")
        
        search_results = web_search_tool.invoke({"query": query_search}, config)

        if isinstance(search_results, dict) and "results" in search_results:
            results = search_results["results"]
        else:
            results = []

        web_documents = []
        for i, res in enumerate(results):
            # L·∫•y c√°c tr∆∞·ªùng c·∫ßn thi·∫øt, ∆∞u ti√™n content, title, url
            content = res.get("content", "")
            title = res.get("title", "")
            url = res.get("url", "")
            # G·ªôp l·∫°i th√†nh 1 ƒëo·∫°n ng·∫Øn g·ªçn, c√≥ th·ªÉ c·∫Øt ng·∫Øn n·∫øu c·∫ßn
            doc_text = f"{title}\n{content}\n{url}".strip()
            max_len = 1500
            doc_text = doc_text[:max_len]
            web_documents.append((f"web_{i}", {"content": doc_text}, 1.0))

        logging.info(f"Found {len(web_documents)} results from web search.")

        return {
            "documents": web_documents,
            "web_search_attempted": True,
            "search_attempts": state.get("search_attempts", 0) + 1,
        }

    def generate(state: RagState, config: RunnableConfig):
        logging.info("---NODE: GENERATE---")
        current_question = get_current_user_question(state)
        documents_count = len(state.get("documents", []))
        logging.debug(f"generate->current_question -> {current_question}")
        logging.debug(f"generate->documents_count -> {documents_count}")
        
        try:
            generation = generation_assistant(state, config)
        except Exception as e:
            user_id = state.get("user", {}).get("user_info", {}).get("user_id", "unknown")
            log_exception_details(
                exception=e,
                context=f"Generate node failure for question: {current_question[:100]}",
                user_id=user_id
            )
            # Return error response
            generation = {"messages": [{"role": "assistant", "content": "Xin l·ªói, c√≥ l·ªói x·∫£y ra khi t·∫°o c√¢u tr·∫£ l·ªùi. Vui l√≤ng th·ª≠ l·∫°i."}]}
        
        return {"messages": [generation]}

    def hallucination_grader_node(state: RagState, config: RunnableConfig):
        logging.info("---NODE: HALLUCINATION GRADER---")
        current_question = get_current_user_question(state)
        generation_message = state["messages"][-1]
        
        if not state.get("documents") or hasattr(generation_message, "tool_calls"):
            return {"hallucination_score": "grounded"}
            
        score = hallucination_grader_assistant(state, config)
        logging.info(f"---HALLUCINATION SCORE: {score.binary_score.upper()}---")
        
        grading_result = "grounded" if score.binary_score.lower() == "yes" else "not_grounded"
        
        update = {
            "hallucination_score": grading_result
        }
        if update["hallucination_score"] == "not_grounded" and state.get(
            "web_search_attempted", False
        ):
            update["force_suggest"] = True
        
        logging.debug(f"hallucination_grader_node->update:{update}")
        return update

    def generate_direct_node(state: RagState, config: RunnableConfig):
        logging.info("---NODE: GENERATE DIRECT---")
        current_question = get_current_user_question(state)
        
        # Check if this is a re-entry from tools (to avoid duplicate reasoning steps)
        messages = state.get("messages", [])
        is_tool_reentry = len(messages) > 0 and isinstance(messages[-1], ToolMessage)
        
        response = direct_answer_assistant(state, config)
        
        return {"messages": [response]}

    def force_suggest_node(state: RagState, config: RunnableConfig):
        logging.info("---NODE: FORCE SUGGEST---")
        current_question = get_current_user_question(state)
        
        response = suggestive_assistant(state, config)

        return {
            "messages": [response],
            "skip_hallucination": True,
            "force_suggest": False,
        }

    def process_document_node(state: RagState, config: RunnableConfig):
        """Process documents/images using specialized document processing assistant.
        
        This node handles:
        1. Extract image/document URLs from attachment metadata 
        2. Analyze content using image_processing_service
        3. Generate contextual response using document_processing_assistant
        4. Handle follow-up questions about analyzed content
        """
        logging.info("---NODE: PROCESS DOCUMENT---")
        
        # Use consistent question extraction method like other nodes
        current_question = get_current_user_question(state)
        user_id = state.get("user_id", "")
        messages = state.get("messages", [])
        
        logging.debug(f"process_document_node->current_question -> {current_question}")
        logging.debug(f"process_document_node->user_id -> {user_id}")
        logging.debug(f"process_document_node->messages_count -> {len(messages)}")
        
        # Validate input like other nodes
        if not current_question or current_question == "C√¢u h·ªèi kh√¥ng r√µ r√†ng":
            logging.warning("process_document_node: Invalid or empty question")
            from langchain_core.messages import AIMessage
            fallback_response = AIMessage(
                content="Xin l·ªói, em kh√¥ng nh·∫≠n ƒë∆∞·ª£c c√¢u h·ªèi r√µ r√†ng. "
                        "Anh/ch·ªã vui l√≤ng g·ª≠i l·∫°i tin nh·∫Øn ho·∫∑c h√¨nh ·∫£nh c·∫ßn h·ªó tr·ª£."
            )
            return {"messages": [fallback_response]}
        
        logging.info(f"Processing document/image query: {current_question[:100]}...")
        
        try:
            # Check if this is a re-entry from tools (consistent with other nodes)
            is_tool_reentry = len(messages) > 0 and isinstance(messages[-1], ToolMessage)
            if is_tool_reentry:
                logging.debug("process_document_node: Tool re-entry detected")
            
            # Extract image URLs from message content
            image_analysis_results = []
            
            # Look for attachment metadata patterns like [H√åNH ·∫¢NH] URL: ...
            import re
            url_patterns = [
                r'\[H√åNH ·∫¢NH\] URL: (https?://[^\s]+)',
                r'\[VIDEO\] URL: (https?://[^\s]+)', 
                r'\[T·ªÜP TIN\] URL: (https?://[^\s]+)',
                r'üì∏.*?(https?://[^\s]+)'  # Legacy format support
            ]
            
            image_urls = []
            for pattern in url_patterns:
                matches = re.findall(pattern, current_question)
                image_urls.extend(matches)

            # Short-circuit if no URLs found to avoid unnecessary processing
            if not image_urls:
                logging.info("No attachment URLs found in current message; skipping document analysis")
                from langchain_core.messages import AIMessage
                response = AIMessage(
                    content="Em ch∆∞a th·∫•y t·ªáp/h√¨nh ·∫£nh n√†o trong tin nh·∫Øn n√†y. Anh/ch·ªã c√≥ th·ªÉ g·ª≠i l·∫°i ·∫£nh ho·∫∑c t·ªáp c·∫ßn ph√¢n t√≠ch kh√¥ng ·∫°?"
                )
                return {"messages": [response]}
            
            # Get image processing service
            image_service = get_image_processing_service()
            
            # Analyze each image URL found
            for url in image_urls:
                logging.info(f"üñºÔ∏è Analyzing image URL: {url[:50]}...")
                try:
                    # Run async image analysis safely in sync context
                    import asyncio
                    import concurrent.futures
                    
                    def run_image_analysis():
                        """Run image analysis in a separate thread with its own event loop"""
                        # Create new event loop for this thread
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(
                                image_service.analyze_image_from_url(
                                    url, 
                                    "H√¨nh ·∫£nh ƒë∆∞·ª£c g·ª≠i b·ªüi kh√°ch h√†ng c·ªßa nh√† h√†ng Tian Long"
                                )
                            )
                        finally:
                            new_loop.close()
                    
                    # Execute in thread pool to avoid blocking current thread
                    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                        future = executor.submit(run_image_analysis)
                        analysis_result = future.result(timeout=30)  # 30s timeout
                    
                    image_analysis_results.append(analysis_result)
                    logging.info(f"‚úÖ Image analysis completed: {analysis_result[:100]}...")
                except Exception as e:
                    logging.error(f"‚ùå Image analysis failed for {url}: {e}")
                    image_analysis_results.append(f"Kh√¥ng th·ªÉ ph√¢n t√≠ch h√¨nh ·∫£nh t·ª´ URL: {url}")
            
            # Prepare enhanced question with image analysis
            enhanced_question = current_question
            if image_analysis_results:
                analysis_text = "\n\n".join(image_analysis_results)
                enhanced_question = f"{current_question}\n\nüì∏ **Ph√¢n t√≠ch h√¨nh ·∫£nh:**\n{analysis_text}"
                logging.info(f"üìù Enhanced question with image analysis: {enhanced_question[:200]}...")
            
            # Update state with enhanced question
            enhanced_state = {**state, "question": enhanced_question}
            
            # Use document processing assistant to generate response
            response = document_processing_assistant(enhanced_state, config)
            
            logging.info("‚úÖ Document/image processing completed successfully")
            return {"messages": [response]}
            
        except Exception as e:
            # Consistent error handling pattern like other nodes
            user_context = state.get("user", {}).get("user_info", {}).get("user_id", "unknown")
            log_exception_details(
                exception=e,
                context=f"Process document node failure for question: {current_question[:100]}",
                user_id=user_context
            )
            
            # Fallback response with consistent messaging
            from langchain_core.messages import AIMessage
            fallback_response = AIMessage(
                content="Xin l·ªói, c√≥ l·ªói x·∫£y ra khi x·ª≠ l√Ω h√¨nh ·∫£nh/t√†i li·ªáu. "
                        "Anh/ch·ªã vui l√≤ng th·ª≠ l·∫°i ho·∫∑c g·ªçi hotline 1900 636 886 ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£."
            )
            return {"messages": [fallback_response]}

    # --- Conditional Edges ---

    def decide_after_grade(
        state: RagState,
    ) -> Literal["rewrite", "generate", "force_suggest"]:
        if state["documents"]:
            return "generate"
        if (
            state.get("search_attempts", 0) >= 2
            and len(state.get("documents", [])) == 0
        ):
            return "force_suggest"
        if state.get("rewrite_count", 0) < 1:
            return "rewrite"
        return "generate"

    def decide_after_hallucination(
        state: RagState,
    ) -> Literal["rewrite", "tools", "end", "generate"]:
        if state.get("force_suggest", False):
            return "generate"
        if state.get("hallucination_score") == "not_grounded":
            return "rewrite" if state.get("rewrite_count", 0) < 1 else "end"
        last_message = state["messages"][-1]
        return (
            "tools"
            if hasattr(last_message, "tool_calls") and last_message.tool_calls
            else "end"
        )

    # Restored: decide next step after generate_direct
    def decide_after_direct_generation(
        state: RagState,
    ) -> Literal["direct_tools", "__end__"]:
        last_message = state["messages"][-1]
        return (
            "direct_tools"
            if hasattr(last_message, "tool_calls") and last_message.tool_calls
            else "__end__"
        )

    # Restored: decide next step after process_document
    def decide_after_process_document(
        state: RagState,
    ) -> Literal["direct_tools", "__end__"]:
        last_message = state["messages"][-1]
        return (
            "direct_tools"
            if hasattr(last_message, "tool_calls") and last_message.tool_calls
            else "__end__"
        )

    # Restored: decide where to go after direct_tools
    def decide_after_direct_tools(
        state: RagState,  
    ) -> Literal["generate_direct", "process_document"]:
        # Heuristic based on question indicators
        question = state.get("question", "").lower()
        if any(ind in question for ind in ["üì∏", "document", "t√†i li·ªáu", "file", "h√¨nh ·∫£nh", "·∫£nh"]):
            logging.info("Returning to process_document after tools")
            return "process_document"
        logging.info("Returning to generate_direct after tools")
        return "generate_direct"

    def decide_entry(
        state: RagState,
    ) -> Literal["retrieve", "web_search", "direct_answer", "process_document"]:
        """Route questions to appropriate processing nodes based on content analysis.
        
        Priority routing logic tightened to avoid false positives:
        - Only route to process_document when the current message contains explicit
          attachment metadata (current-turn) or the pre‚Äëanalyzed marker.
        - Otherwise, honor the router decision (vectorstore/web_search/direct_answer).
        """
        question_raw = state.get("question", "")
        question = question_raw.lower()
        datasource = state.get("datasource", "direct_answer")
        
        logging.debug(f"decide_entry->question: {question[:100]}...")
        logging.debug(f"decide_entry->router_datasource: {datasource}")

        # Guard against false 'process_document' when no current-turn attachments
        if datasource == "process_document" and not _has_attachment_metadata(question_raw):
            logging.info("üõë Router chose process_document but no current-turn attachments detected -> override to direct_answer")
            return "direct_answer"
        
        # Map router decisions to valid node names
        if datasource == "vectorstore":
            logging.info(f"üîÄ Router decision: vectorstore ‚Üí retrieve")
            return "vectorstore"
        elif datasource == "web_search":
            logging.info(f"üîÄ Router decision: web_search ‚Üí web_search")
            return "web_search"
        elif datasource == "process_document":
            logging.info(f"üñºÔ∏è Router decision: process_document ‚Üí process_document")
            return "process_document"
        else:  # direct_answer or any other case
            logging.info(f"üîÄ Router decision: {datasource} ‚Üí direct_answer")
            return "direct_answer"

    # --- Build the Graph ---
    graph = StateGraph(RagState)

    summarization_node = SummarizationNode(
        token_counter=count_tokens_approximately,
        model=llm_summarizer,
        max_tokens=1200,
        max_tokens_before_summary=1000,
        max_summary_tokens=800,
    )

    graph.add_node("user_info", user_info)
    graph.add_node("summarizer", summarization_node)
   
    graph.add_node("router", route_question)
    graph.add_node("retrieve", retrieve)
    graph.add_node("grade_documents", grade_documents_node)
    graph.add_node("rewrite", rewrite)
    graph.add_node("web_search", web_search_node)
    graph.add_node("generate", generate)
    graph.add_node("hallucination_grader", hallucination_grader_node)
    graph.add_node("force_suggest", force_suggest_node)
    graph.add_node("generate_direct", generate_direct_node)
    graph.add_node("process_document", process_document_node)
    graph.add_node("tools", ToolNode(tools=all_tools))
    graph.add_node("direct_tools", ToolNode(tools=memory_tools + tools + image_tools))

    # --- Define Graph Flow ---
    graph.set_entry_point("user_info")

    # graph.add_conditional_edges(
    #     "user_info",
    #     should_summarize,
    #     {"summarize": "summarizer", "continue": "contextualize_question"},
    # )
    # graph.add_edge("summarizer", "contextualize_question")
    # FIXED: Conditional summarization - only summarize when needed
    # Add conditional edge from user_info to decide whether to summarize or go directly to router
    graph.add_conditional_edges(
        "user_info",
        lambda state: "summarize" if should_summarize_conversation(state) else "continue",
        {"summarize": "summarizer", "continue": "router"},
    )
    graph.add_edge("summarizer", "router")
    graph.add_conditional_edges(
        "router",
        decide_entry,
        {
            "vectorstore": "retrieve",
            "web_search": "web_search",
            "direct_answer": "generate_direct",
            "process_document": "process_document",
        },
    )
    graph.add_edge("retrieve", "grade_documents")
    graph.add_conditional_edges(
        "grade_documents",
        decide_after_grade,
        {
            "rewrite": "rewrite",
            "generate": "generate",
            "force_suggest": "force_suggest",
        },
    )
    graph.add_edge("rewrite", "retrieve")
    graph.add_edge("web_search", "grade_documents")
    # graph.add_edge("force_suggest", "generate")
    graph.add_edge("force_suggest", END)
    
    # Process document node with conditional edge for tool calls
    graph.add_conditional_edges(
        "process_document",
        decide_after_process_document,
        {"direct_tools": "direct_tools", "__end__": END},
    )
    graph.add_conditional_edges(
        "generate",
        lambda s: "hallucination_grader" if not s.get("skip_hallucination") else END,
        {"hallucination_grader": "hallucination_grader", END: END},
    )
    graph.add_conditional_edges(
        "hallucination_grader",
        decide_after_hallucination,
        {"rewrite": "rewrite", "tools": "tools", "end": END, "generate": "generate"},
    )
    graph.add_edge("tools", "generate")

    graph.add_conditional_edges(
        "generate_direct",
        decide_after_direct_generation,
        {"direct_tools": "direct_tools", "__end__": END},
    )
    # Direct tools can route back to either generate_direct or process_document
    graph.add_conditional_edges(
        "direct_tools",
        decide_after_direct_tools,
        {"generate_direct": "generate_direct", "process_document": "process_document"},
    )

    return graph


# --- Singleton Pattern for Compiled Graph ---
_adaptive_rag_app_instance = None
_adaptive_rag_checkpointer = None


def compile_adaptive_rag_graph_with_checkpointing(
    checkpointer, uncompiled_graph: StateGraph
):
    global _adaptive_rag_app_instance, _adaptive_rag_checkpointer
    if (
        _adaptive_rag_app_instance is not None
        and _adaptive_rag_checkpointer is checkpointer
    ):
        return _adaptive_rag_app_instance
    _adaptive_rag_checkpointer = checkpointer
    _adaptive_rag_app_instance = uncompiled_graph.compile(checkpointer=checkpointer)

    return _adaptive_rag_app_instance
