# PhÃ¢n TÃ­ch Human-in-the-Loop vÃ  Luá»“ng Book Ticket

## ğŸ“‹ TÃ³m Táº¯t Bá»‘i Cáº£nh

Hiá»‡n táº¡i, khi user click button **Cancel** trong confirmation dialog, thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c gá»­i lÃªn server vÃ  server Ä‘Ã£ pháº£n há»“i láº¡i, nhÆ°ng cá»­a sá»• confirm váº«n tiáº¿p tá»¥c hiá»‡n ra. ÄÃ¢y lÃ  má»™t ká»‹ch báº£n khÃ´ng mong muá»‘n.

**Input Ä‘Æ°á»£c gá»­i lÃªn server khi báº¥m Cancel:**
```json
{
  "input": {
    "cancel_action": {
      "user_action": "cancel",
      "cancelled_tools": [],
      "timestamp": "2025-07-22T05:32:34.643Z", 
      "reason": "User cancelled the operation"
    }
  },
  "stream_mode": ["values", "messages", "custom"],
  "stream_subgraphs": true,
  "assistant_id": "agent"
}
```

## ğŸ” PhÃ¢n TÃ­ch LangGraph Human-in-the-Loop

### CÃ¡ch Thá»©c Hoáº¡t Äá»™ng Chuáº©n cá»§a LangGraph

Theo tÃ i liá»‡u LangGraph, human-in-the-loop Ä‘Æ°á»£c implement qua:

#### 1. **Interrupt Mechanism**
```python
from langgraph.types import interrupt, Command

def tool_approval_node(state):
    response = interrupt({
        "question": "Do you approve this tool call?",
        "tool_call": state["pending_tool_call"]
    })
    
    if response == "approve":
        return Command(goto="execute_tool")
    else:
        return Command(goto="cancel_tool")
```

#### 2. **Resume vá»›i Command**
```python
# Approve
graph.invoke(Command(resume="approve"), config=config)

# Reject  
graph.invoke(Command(resume="reject"), config=config)
```

#### 3. **Interrupt Before Tool Execution**
LangGraph sá»­ dá»¥ng `interrupt_before` Ä‘á»ƒ pause graph trÆ°á»›c khi execute sensitive tools:

```python
graph = builder.compile(
    checkpointer=checkpointer,
    interrupt_before=[
        "sensitive_tool_node"
    ]
)
```

## ğŸ”„ PhÃ¢n TÃ­ch Luá»“ng Hiá»‡n Táº¡i

### Client Side (Frontend)

#### 1. **useThreadLogic.ts - onCancel**
```typescript
const onCancel = useCallback(async () => {
  // 1. Hide interrupt dialog
  setInterrupt(null);
  
  // 2. Show reasoning window  
  setIsLoading(true);
  setReasoningSteps([]);
  
  // 3. Send cancel signal
  const cancelSignal = {
    user_action: "cancel",
    cancelled_tools: interrupt?.tool_calls?.map((tc: any) => tc.name) || [],
    timestamp: new Date().toISOString(),
    reason: "User cancelled the operation"
  };
  
  // 4. Stream to backend
  const stream = client.runs.stream(localThreadId, "agent", {
    input: { cancel_action: cancelSignal },
    streamSubgraphs: true,
    streamMode: ["values", "messages", "custom"],
  });
  
  // 5. Process response
  for await (const chunk of stream) {
    processStreamingChunk(chunk);
  }
}, [...]);
```

#### 2. **InterruptView.tsx**
- Hiá»ƒn thá»‹ confirmation dialog
- CÃ³ 2 buttons: Continue vÃ  Cancel
- Gá»i `onCancel()` khi user click Cancel

#### 3. **Reducer - processStreamingChunk**
- Xá»­ lÃ½ streaming response tá»« server
- Update messages vÃ  reasoning steps

### Server Side (Backend)

#### 1. **travel_graph.py - Interrupt Configuration**
```python
_graph_instance = builder.compile(
    store=qdrant_store,
    interrupt_before=[
        "flight_assistant_sensitive_tools",
        "book_car_rental_sensitive_tools", 
        "book_hotel_sensitive_tools",
        "book_excursion_sensitive_tools",
    ],
)
```

#### 2. **Assistant Class - __call__ method**
```python
def __call__(self, state: State, config: RunnableConfig):
    # Process state including cancel_action
    # Generate LLM response
    result = self.runnable.invoke(self.binding_prompt(state), config)
    return {"messages": result, "reasoning_steps": [reasoning_step]}
```

#### 3. **Tools vá»›i interrupt_before**
- `book_ticket`, `cancel_ticket`, `update_ticket_to_new_flight`
- `book_hotel`, `cancel_hotel`, `update_hotel`  
- `book_car_rental`, `cancel_car_rental`
- `book_excursion`, `cancel_excursion`

## âŒ Váº¥n Äá» Hiá»‡n Táº¡i

### 1. **CÆ¡ Cháº¿ KhÃ´ng ÄÃºng Chuáº©n LangGraph**

**Hiá»‡n táº¡i:**
- Client gá»­i custom `cancel_action` object
- Server nháº­n vÃ  pháº£n há»“i, nhÆ°ng interrupt state khÃ´ng Ä‘Æ°á»£c clear
- Dialog tiáº¿p tá»¥c hiá»ƒn thá»‹

**Chuáº©n LangGraph:**
- Sá»­ dá»¥ng `Command(resume=...)` Ä‘á»ƒ approve/reject
- Graph tá»± Ä‘á»™ng resume tá»« interrupt point
- Interrupt state Ä‘Æ°á»£c clear sau khi resume

### 2. **Luá»“ng Xá»­ LÃ½ KhÃ´ng Thá»‘ng Nháº¥t**

```mermaid
graph TD
    A[User Click Cancel] --> B[setInterrupt(null)]
    B --> C[Send cancel_action]
    C --> D[Server Process]
    D --> E[Server Response]
    E --> F[processStreamingChunk]
    F --> G[Interrupt váº«n hiá»ƒn thá»‹ âŒ]
```

### 3. **Thiáº¿u Command Primitive**
- KhÃ´ng sá»­ dá»¥ng `Command(resume=...)` 
- KhÃ´ng follow LangGraph interrupt/resume pattern

## âœ… Giáº£i PhÃ¡p Äá» Xuáº¥t

### PhÆ°Æ¡ng Ãn 1: Sá»­ dá»¥ng Chuáº©n LangGraph Command

#### Client Changes:

**1. Update onCancel to use Command:**
```typescript
const onCancel = useCallback(async () => {
  setInterrupt(null);
  setIsLoading(true);
  setReasoningSteps([]);
  
  // Use Command primitive instead of custom input
  const stream = client.runs.stream(localThreadId, "agent", {
    input: Command({ resume: "reject" }), // Standard LangGraph way
    streamSubgraphs: true,
    streamMode: ["values", "messages", "custom"],
  });
  
  for await (const chunk of stream) {
    processStreamingChunk(chunk);
  }
}, [...]);
```

**2. Update onContinue similarly:**
```typescript  
const onContinue = useCallback(async () => {
  setInterrupt(null);
  setIsLoading(true);
  
  const stream = client.runs.stream(localThreadId, "agent", {
    input: Command({ resume: "approve" }), // Standard approve
    streamSubgraphs: true,
    streamMode: ["values", "messages", "custom"],
  });
  
  for await (const chunk of stream) {
    processStreamingChunk(chunk);
  }
}, [...]);
```

#### Server Changes:

**1. Update Assistant to handle Command properly:**
```python
def __call__(self, state: State, config: RunnableConfig):
    # Check if this is a resume from interrupt
    if hasattr(state, '__resume__'):
        resume_value = state['__resume__']
        if resume_value == "reject":
            # Handle cancellation
            return {
                "messages": [AIMessage(content="Action cancelled by user.")],
                "reasoning_steps": [create_reasoning_step("assistant", "User cancelled the action")]
            }
        elif resume_value == "approve":
            # Continue with normal execution
            pass
    
    # Normal processing
    result = self.runnable.invoke(self.binding_prompt(state), config)
    return {"messages": result, "reasoning_steps": [reasoning_step]}
```

**2. Add proper interrupt handling in sensitive tool nodes:**
```python
def review_tool_call_node(state: State):
    """Node to review sensitive tool calls before execution"""
    pending_tools = state.get("pending_tool_calls", [])
    
    if not pending_tools:
        return state
    
    # Show tool calls for review
    response = interrupt({
        "type": "tool_confirmation", 
        "tool_calls": pending_tools,
        "message": "Please review the tool calls before execution"
    })
    
    # This will be handled by Command(resume=...) from client
    return state
```

### PhÆ°Æ¡ng Ãn 2: Fix Current Implementation

#### Client Changes:

**1. Ensure interrupt state is properly cleared:**
```typescript
// In processStreamingChunk reducer
case "PROCESS_STREAMING_CHUNK": {
  const chunk = action.payload;
  
  // If we get a response after cancel, clear interrupt
  if (chunk.event === "messages/complete" && state.interrupt) {
    console.log("ğŸ”§ Clearing interrupt after cancel response");
    return {
      ...state,
      messages: chunk.data.messages,
      interrupt: null, // Force clear interrupt
      isLoading: false
    };
  }
  
  // ... rest of processing
}
```

**2. Add explicit interrupt clearing in onCancel:**
```typescript
const onCancel = useCallback(async () => {
  console.log("ğŸ”§ onCancel - Force clearing interrupt");
  
  // Force clear interrupt in multiple ways
  setInterrupt(null);
  actions.setInterrupt(null);
  
  // Continue with existing logic...
}, [...]);
```

#### Server Changes:

**1. Handle cancel_action in Assistant:**
```python
def __call__(self, state: State, config: RunnableConfig):
    # Check for cancel_action in input
    cancel_action = state.get("cancel_action")
    if cancel_action and cancel_action.get("user_action") == "cancel":
        # Return cancellation response
        return {
            "messages": [AIMessage(
                content="Booking cancelled by user request. No action was taken."
            )],
            "reasoning_steps": [
                create_reasoning_step(
                    "assistant", 
                    "âœ… User cancelled booking - no action taken",
                    {"cancelled_tools": cancel_action.get("cancelled_tools", [])}
                )
            ]
        }
    
    # Normal processing...
```

## ğŸ“ BÆ°á»›c Thá»±c Hiá»‡n Chi Tiáº¿t

### Giai Äoáº¡n 1: Chá»n PhÆ°Æ¡ng Ãn

**Khuyáº¿n nghá»‹: PhÆ°Æ¡ng Ãn 1** (Sá»­ dá»¥ng chuáº©n LangGraph Command)
- ÄÃºng chuáº©n LangGraph
- Dá»… maintain
- Consistent vá»›i LangGraph ecosystem

### Giai Äoáº¡n 2: Implement Changes

#### Step 1: Update Client useThreadLogic.ts
```typescript
// Replace current onCancel/onContinue with Command-based approach
```

#### Step 2: Update Server Assistant Class  
```python
# Add proper Command resume handling
```

#### Step 3: Test Flow
1. Trigger tool confirmation
2. Test Cancel â†’ Should show cancellation message
3. Test Continue â†’ Should execute tool
4. Verify interrupt dialog disappears in both cases

#### Step 4: Update Error Handling
- Add proper error handling for Command failures
- Add timeout handling for interrupt/resume

### Giai Äoáº¡n 3: Testing & Validation

#### Test Cases:
1. **Happy Path Continue**: User approves â†’ Tool executes
2. **Happy Path Cancel**: User cancels â†’ Cancellation message  
3. **Network Error**: Handle connection failures
4. **Multiple Interrupts**: Handle concurrent interrupts
5. **Session Timeout**: Handle expired threads

## ğŸš€ Expected Results

Sau khi implement:

1. âœ… **Cancel hoáº¡t Ä‘á»™ng Ä‘Ãºng**: Dialog áº©n, hiá»ƒn thá»‹ cancellation message
2. âœ… **Continue hoáº¡t Ä‘á»™ng Ä‘Ãºng**: Dialog áº©n, tool execute, hiá»ƒn thá»‹ result  
3. âœ… **UI consistent**: Reasoning window hiá»ƒn thá»‹ Ä‘Ãºng
4. âœ… **Code maintainable**: Follow LangGraph standards
5. âœ… **Error handling**: Robust error handling

## ğŸ“Š Káº¿t Luáº­n

Váº¥n Ä‘á» hiá»‡n táº¡i lÃ  do **khÃ´ng follow Ä‘Ãºng chuáº©n LangGraph Command/Interrupt pattern**. Giáº£i phÃ¡p tá»‘t nháº¥t lÃ  refactor Ä‘á»ƒ sá»­ dá»¥ng `Command(resume=...)` thay vÃ¬ custom `cancel_action` input.

Äiá»u nÃ y sáº½ Ä‘áº£m báº£o:
- Interrupt state Ä‘Æ°á»£c manage Ä‘Ãºng cÃ¡ch bá»Ÿi LangGraph
- UI behavior consistent vÃ  predictable  
- Code dá»… maintain vÃ  scale
- Compatible vá»›i LangGraph ecosystem
