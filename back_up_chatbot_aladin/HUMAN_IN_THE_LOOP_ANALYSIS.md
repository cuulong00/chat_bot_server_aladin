# Ph√¢n T√≠ch Human-in-the-Loop v√† Lu·ªìng Book Ticket

## üìã T√≥m T·∫Øt B·ªëi C·∫£nh

Hi·ªán t·∫°i, khi user click button **Cancel** trong confirmation dialog, th√¥ng tin ƒë√£ ƒë∆∞·ª£c g·ª≠i l√™n server v√† server ƒë√£ ph·∫£n h·ªìi l·∫°i, nh∆∞ng c·ª≠a s·ªï confirm v·∫´n ti·∫øp t·ª•c hi·ªán ra. ƒê√¢y l√† m·ªôt k·ªãch b·∫£n kh√¥ng mong mu·ªën.

**Input ƒë∆∞·ª£c g·ª≠i l√™n server khi b·∫•m Cancel:**
```json
{
  "input": {
    "cancel_action": {
      "user_action": "cancel",
      "cancelled_tools": [],
      "timestamp": "2025-07-22T05:32:34.643Z", 
      "reason": "User cancelled the operation"
    }
  },
  "stream_mode": ["values", "messages", "custom"],
  "stream_subgraphs": true,
  "assistant_id": "agent"
}
```

## üîç Ph√¢n T√≠ch LangGraph Human-in-the-Loop

### C√°ch Th·ª©c Ho·∫°t ƒê·ªông Chu·∫©n c·ªßa LangGraph

Theo t√†i li·ªáu LangGraph, human-in-the-loop ƒë∆∞·ª£c implement qua:

#### 1. **Interrupt Mechanism**
```python
from langgraph.types import interrupt, Command

def tool_approval_node(state):
    response = interrupt({
        "question": "Do you approve this tool call?",
        "tool_call": state["pending_tool_call"]
    })
    
    if response == "approve":
        return Command(goto="execute_tool")
    else:
        return Command(goto="cancel_tool")
```

#### 2. **Resume v·ªõi Command**
```python
# Approve
graph.invoke(Command(resume="approve"), config=config)

# Reject  
graph.invoke(Command(resume="reject"), config=config)
```

#### 3. **Interrupt Before Tool Execution**
LangGraph s·ª≠ d·ª•ng `interrupt_before` ƒë·ªÉ pause graph tr∆∞·ªõc khi execute sensitive tools:

```python
graph = builder.compile(
    checkpointer=checkpointer,
    interrupt_before=[
        "sensitive_tool_node"
    ]
)
```

## üîÑ Ph√¢n T√≠ch Lu·ªìng Hi·ªán T·∫°i

### Client Side (Frontend)

#### 1. **useThreadLogic.ts - onCancel**
```typescript
const onCancel = useCallback(async () => {
  // 1. Hide interrupt dialog
  setInterrupt(null);
  
  // 2. Show reasoning window  
  setIsLoading(true);
  setReasoningSteps([]);
  
  // 3. Send cancel signal
  const cancelSignal = {
    user_action: "cancel",
    cancelled_tools: interrupt?.tool_calls?.map((tc: any) => tc.name) || [],
    timestamp: new Date().toISOString(),
    reason: "User cancelled the operation"
  };
  
  // 4. Stream to backend
  const stream = client.runs.stream(localThreadId, "agent", {
    input: { cancel_action: cancelSignal },
    streamSubgraphs: true,
    streamMode: ["values", "messages", "custom"],
  });
  
  // 5. Process response
  for await (const chunk of stream) {
    processStreamingChunk(chunk);
  }
}, [...]);
```

#### 2. **InterruptView.tsx**
- Hi·ªÉn th·ªã confirmation dialog
- C√≥ 2 buttons: Continue v√† Cancel
- G·ªçi `onCancel()` khi user click Cancel

#### 3. **Reducer - processStreamingChunk**
- X·ª≠ l√Ω streaming response t·ª´ server
- Update messages v√† reasoning steps

### Server Side (Backend)

#### 1. **travel_graph.py - Interrupt Configuration**
```python
_graph_instance = builder.compile(
    store=qdrant_store,
    interrupt_before=[
        "flight_assistant_sensitive_tools",
        "book_car_rental_sensitive_tools", 
        "book_hotel_sensitive_tools",
        "book_excursion_sensitive_tools",
    ],
)
```

#### 2. **Assistant Class - __call__ method**
```python
def __call__(self, state: State, config: RunnableConfig):
    # Process state including cancel_action
    # Generate LLM response
    result = self.runnable.invoke(self.binding_prompt(state), config)
    return {"messages": result, "reasoning_steps": [reasoning_step]}
```

#### 3. **Tools v·ªõi interrupt_before**
- `book_ticket`, `cancel_ticket`, `update_ticket_to_new_flight`
- `book_hotel`, `cancel_hotel`, `update_hotel`  
- `book_car_rental`, `cancel_car_rental`
- `book_excursion`, `cancel_excursion`

## ‚ùå V·∫•n ƒê·ªÅ Hi·ªán T·∫°i

### 1. **C∆° Ch·∫ø Kh√¥ng ƒê√∫ng Chu·∫©n LangGraph**

**Hi·ªán t·∫°i:**
- Client g·ª≠i custom `cancel_action` object
- Server nh·∫≠n v√† ph·∫£n h·ªìi, nh∆∞ng interrupt state kh√¥ng ƒë∆∞·ª£c clear
- Dialog ti·∫øp t·ª•c hi·ªÉn th·ªã

**Chu·∫©n LangGraph:**
- S·ª≠ d·ª•ng `Command(resume=...)` ƒë·ªÉ approve/reject
- Graph t·ª± ƒë·ªông resume t·ª´ interrupt point
- Interrupt state ƒë∆∞·ª£c clear sau khi resume

### 2. **Lu·ªìng X·ª≠ L√Ω Kh√¥ng Th·ªëng Nh·∫•t**

```mermaid
graph TD
    A[User Click Cancel] --> B[setInterrupt(null)]
    B --> C[Send cancel_action]
    C --> D[Server Process]
    D --> E[Server Response]
    E --> F[processStreamingChunk]
    F --> G[Interrupt v·∫´n hi·ªÉn th·ªã ‚ùå]
```

### 3. **Thi·∫øu Command Primitive**
- Kh√¥ng s·ª≠ d·ª•ng `Command(resume=...)` 
- Kh√¥ng follow LangGraph interrupt/resume pattern

## ‚úÖ Gi·∫£i Ph√°p ƒê·ªÅ Xu·∫•t

### Ph∆∞∆°ng √Ån 1: S·ª≠ d·ª•ng Chu·∫©n LangGraph Command

#### Client Changes:

**1. Update onCancel to use Command:**
```typescript
const onCancel = useCallback(async () => {
  setInterrupt(null);
  setIsLoading(true);
  setReasoningSteps([]);
  
  // Use Command primitive instead of custom input
  const stream = client.runs.stream(localThreadId, "agent", {
    input: Command({ resume: "reject" }), // Standard LangGraph way
    streamSubgraphs: true,
    streamMode: ["values", "messages", "custom"],
  });
  
  for await (const chunk of stream) {
    processStreamingChunk(chunk);
  }
}, [...]);
```

**2. Update onContinue similarly:**
```typescript  
const onContinue = useCallback(async () => {
  setInterrupt(null);
  setIsLoading(true);
  
  const stream = client.runs.stream(localThreadId, "agent", {
    input: Command({ resume: "approve" }), // Standard approve
    streamSubgraphs: true,
    streamMode: ["values", "messages", "custom"],
  });
  
  for await (const chunk of stream) {
    processStreamingChunk(chunk);
  }
}, [...]);
```

#### Server Changes:

**1. Update Assistant to handle Command properly:**
```python
def __call__(self, state: State, config: RunnableConfig):
    # Check if this is a resume from interrupt
    if hasattr(state, '__resume__'):
        resume_value = state['__resume__']
        if resume_value == "reject":
            # Handle cancellation
            return {
                "messages": [AIMessage(content="Action cancelled by user.")],
                "reasoning_steps": [create_reasoning_step("assistant", "User cancelled the action")]
            }
        elif resume_value == "approve":
            # Continue with normal execution
            pass
    
    # Normal processing
    result = self.runnable.invoke(self.binding_prompt(state), config)
    return {"messages": result, "reasoning_steps": [reasoning_step]}
```

**2. Add proper interrupt handling in sensitive tool nodes:**
```python
def review_tool_call_node(state: State):
    """Node to review sensitive tool calls before execution"""
    pending_tools = state.get("pending_tool_calls", [])
    
    if not pending_tools:
        return state
    
    # Show tool calls for review
    response = interrupt({
        "type": "tool_confirmation", 
        "tool_calls": pending_tools,
        "message": "Please review the tool calls before execution"
    })
    
    # This will be handled by Command(resume=...) from client
    return state
```

### Ph∆∞∆°ng √Ån 2: Fix Current Implementation

#### Client Changes:

**1. Ensure interrupt state is properly cleared:**
```typescript
// In processStreamingChunk reducer
case "PROCESS_STREAMING_CHUNK": {
  const chunk = action.payload;
  
  // If we get a response after cancel, clear interrupt
  if (chunk.event === "messages/complete" && state.interrupt) {
    console.log("üîß Clearing interrupt after cancel response");
    return {
      ...state,
      messages: chunk.data.messages,
      interrupt: null, // Force clear interrupt
      isLoading: false
    };
  }
  
  // ... rest of processing
}
```

**2. Add explicit interrupt clearing in onCancel:**
```typescript
const onCancel = useCallback(async () => {
  console.log("üîß onCancel - Force clearing interrupt");
  
  // Force clear interrupt in multiple ways
  setInterrupt(null);
  actions.setInterrupt(null);
  
  // Continue with existing logic...
}, [...]);
```

#### Server Changes:

**1. Handle cancel_action in Assistant:**
```python
def __call__(self, state: State, config: RunnableConfig):
    # Check for cancel_action in input
    cancel_action = state.get("cancel_action")
    if cancel_action and cancel_action.get("user_action") == "cancel":
        # Return cancellation response
        return {
            "messages": [AIMessage(
                content="Booking cancelled by user request. No action was taken."
            )],
            "reasoning_steps": [
                create_reasoning_step(
                    "assistant", 
                    "‚úÖ User cancelled booking - no action taken",
                    {"cancelled_tools": cancel_action.get("cancelled_tools", [])}
                )
            ]
        }
    
    # Normal processing...
```

## üìù B∆∞·ªõc Th·ª±c Hi·ªán Chi Ti·∫øt

### Giai ƒêo·∫°n 1: Ch·ªçn Ph∆∞∆°ng √Ån

**Khuy·∫øn ngh·ªã: Ph∆∞∆°ng √Ån 1** (S·ª≠ d·ª•ng chu·∫©n LangGraph Command)
- ƒê√∫ng chu·∫©n LangGraph
- D·ªÖ maintain
- Consistent v·ªõi LangGraph ecosystem

### Giai ƒêo·∫°n 2: Implement Changes

#### Step 1: Update Client useThreadLogic.ts
```typescript
// Replace current onCancel/onContinue with Command-based approach
```

#### Step 2: Update Server Assistant Class  
```python
# Add proper Command resume handling
```

#### Step 3: Test Flow
1. Trigger tool confirmation
2. Test Cancel ‚Üí Should show cancellation message
3. Test Continue ‚Üí Should execute tool
4. Verify interrupt dialog disappears in both cases

#### Step 4: Update Error Handling
- Add proper error handling for Command failures
- Add timeout handling for interrupt/resume

### Giai ƒêo·∫°n 3: Testing & Validation

#### Test Cases:
1. **Happy Path Continue**: User approves ‚Üí Tool executes
2. **Happy Path Cancel**: User cancels ‚Üí Cancellation message  
3. **Network Error**: Handle connection failures
4. **Multiple Interrupts**: Handle concurrent interrupts
5. **Session Timeout**: Handle expired threads

## üöÄ Expected Results

Sau khi implement:

1. ‚úÖ **Cancel ho·∫°t ƒë·ªông ƒë√∫ng**: Dialog ·∫©n, hi·ªÉn th·ªã cancellation message
2. ‚úÖ **Continue ho·∫°t ƒë·ªông ƒë√∫ng**: Dialog ·∫©n, tool execute, hi·ªÉn th·ªã result  
3. ‚úÖ **UI consistent**: Reasoning window hi·ªÉn th·ªã ƒë√∫ng
4. ‚úÖ **Code maintainable**: Follow LangGraph standards
5. ‚úÖ **Error handling**: Robust error handling

## üìä K·∫øt Lu·∫≠n

V·∫•n ƒë·ªÅ hi·ªán t·∫°i l√† do **kh√¥ng follow ƒë√∫ng chu·∫©n LangGraph Command/Interrupt pattern**. Gi·∫£i ph√°p t·ªët nh·∫•t l√† refactor ƒë·ªÉ s·ª≠ d·ª•ng `Command(resume=...)` thay v√¨ custom `cancel_action` input.

ƒêi·ªÅu n√†y s·∫Ω ƒë·∫£m b·∫£o:
- Interrupt state ƒë∆∞·ª£c manage ƒë√∫ng c√°ch b·ªüi LangGraph
- UI behavior consistent v√† predictable  
- Code d·ªÖ maintain v√† scale
- Compatible v·ªõi LangGraph ecosystem
